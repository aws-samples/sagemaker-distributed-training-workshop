{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune DeepSeek distilled Llama 8b and 70b with PyTorch FSDP on Amazon SageMaker\n",
    "\n",
    "This notebook explains how you can fine-tune the DeepSeek distilled version of Llama 8b mdoel using PyTorch FSDP with the help of Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index) on Amazon SageMaker on medical FreedomIntelligence/medical-o1-reasoning-SFT dataset which is a comprehensive collection of medical reasoning datasets ..\n",
    "\n",
    "**This notebook is validated and optimized to run on `ml.p4d.2xlarge` instances**\n",
    "\n",
    "**FSDP + Q-Lora Background**\n",
    "\n",
    "Hugging Face shares the support of Q-Lora and PyTorch FSDP (Fully Sharded Data Parallel). FSDP and Q-Lora allow you now to fine-tune Llama, Mistral-like architectures. Hugging Face PEFT is where the core logic resides, read more about it in the [PEFT documentation](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp).\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n",
    "\n",
    "This notebook walks you thorugh how to fine-tune open LLMs from Hugging Face using Amazon SageMaker.\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"py7zr\" \"peft==0.12.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from datasets import load_dataset\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.s3 import S3Downloader\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"FreedomIntelligence/medical-o1-reasoning-SFT\"\n",
    "\n",
    "# Provide hf_token value to acccess deepseek model\n",
    "os.environ['hf_token']=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "In this example, we use the FreedomIntelligence/medical-o1-reasoning-SFT dataset from Hugging Face. The FreedomIntelligence/medical-o1-reasoning-SFT is used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning. This dataset is constructed using GPT-4o, which searches for solutions to verifiable medical problems and validates them through a medical verifier.\n",
    "\n",
    "For details, see our paper and GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a medical analysis prompt based on patient information.\n",
    "    \n",
    "    Args:\n",
    "        data_point (dict): Dictionary containing target and meaning_representation keys\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the formatted prompt\n",
    "    \"\"\"\n",
    "    full_prompt = f\"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "    Write a response that appropriately completes the request. \n",
    "    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "    ### Instruction:\n",
    "    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "    Please answer the following medical question. \n",
    "\n",
    "    ### Question:\n",
    "    {data_point[\"Question\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"Complex_CoT\"]}\n",
    "\n",
    "    \"\"\"\n",
    "    return {\"prompt\": full_prompt.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "train_set = load_dataset(dataset_name, 'en', split=\"train[5%:]\")\n",
    "test_set = load_dataset(dataset_name, 'en', split=\"train[:5%]\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(train_set.features)\n",
    "\n",
    "train_dataset = train_set.map(\n",
    "    generate_and_tokenize_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "test_dataset = test_set.map(\n",
    "    generate_and_tokenize_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review dataset\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "input_path = f's3://{sess.default_bucket()}/datasets/deepseek-8b'\n",
    "\n",
    "# Save datasets to s3\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "train_dataset.to_json(f\"{input_path}/train/dataset.json\", orient=\"records\")\n",
    "train_dataset_s3_path = f\"{input_path}/train/dataset.json\"\n",
    "test_dataset.to_json(f\"{input_path}/test/dataset.json\", orient=\"records\")\n",
    "test_dataset_s3_path = f\"{input_path}/test/dataset.json\"\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure input length\n",
    "\n",
    "While passing in a dataset to the LLM for fine-tuning, it's important to ensure that the inputs are all of a uniform length. To achieve this, we first visualize the distribution of the input token lengths (or alternatively, firectly find the max length). Based on these results, we identify the maximum input token length, and utilize \"padding\" to ensure all the inputs are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_lengths(train_dataset, test_dataset):\n",
    "    lengths1 = [count_words(x[\"prompt\"]) for x in train_dataset]\n",
    "    lengths2 = [count_words(x[\"prompt\"]) for x in test_dataset]\n",
    "    lengths = lengths1 + lengths2\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(lengths, bins=100, alpha=0.7, color=\"blue\")\n",
    "    plt.xlim([0,2000])\n",
    "    plt.xlabel(\"prompt lengths\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of lengths of input_ids\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out the max tokens\n",
    "lengths1 = [count_words(x[\"prompt\"]) for x in train_dataset]\n",
    "lengths2 = [count_words(x[\"prompt\"]) for x in test_dataset]\n",
    "lengths = lengths1 + lengths2\n",
    "\n",
    "max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune DeepSeek 8b and 70b on Amazon SageMaker\n",
    "\n",
    "We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers`. We prepared a script [launch_fsdp_qlora.py](../scripts/launch_fsdp_qlora.py) which will loads the dataset from disk, prepare the model, tokenizer and start the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. \n",
    "\n",
    "For configuration we use `TrlParser`, that allows us to provide hyperparameters in a yaml file. This `yaml` will be uploaded and provided to Amazon SageMaker similar to our datasets. Below is the config file for fine-tuning DeepSeek R1 distilled Llama 8B on ml.p4d.24xlarge 40GB GPUs. We are saving the config file as `args.yaml` and upload it to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "hf_token: \"${hf_token}\" # Use HF token to login into Hugging Face to access the DeepSeek distilled models\n",
    "model_id: \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"      # Hugging Face model id, replace it with 70b if needeed\n",
    "max_seq_length: 1024  #512 # 2048               # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "\n",
    "output_dir: \"/opt/ml/model/deepseek/output\"              # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"tensorboard\"              # report metrics to tensorboard\n",
    "learning_rate: 0.0003                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                  # number of training epochs\n",
    "per_device_train_batch_size: 10       # batch size per device during training\n",
    "per_device_eval_batch_size: 8         # batch size for evaluation\n",
    "gradient_accumulation_steps: 2        # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "\n",
    "weight_decay: 0.01\n",
    "warmup_steps: 100\n",
    "# offload FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune\n",
    "\n",
    "Below estimtor will train the model and will save the LoRA adapter in S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'deepseek-8b-finetune'\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point= 'launch_fsdp_qlora.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=10800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/args.yaml\" # path to TRL config which was uploaded to s3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: When using QLoRA, we only train adapters and not the full model. The [launch_fsdp_qlora.py](../scripts/fsdp/run_fsdp_qlora.py) saves the `adapter` at the end of the training to Amazon SageMaker S3 bucket (sagemaker-<region name>-<account_id>)._\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'test': test_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    "\n",
    "# Check input channels configured \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine the job name of the last run or you can browse the console\n",
    "latest_run_job_name= pytorch_estimator.latest_training_job.job_name\n",
    "latest_run_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Steps are taken by the next estimator:\n",
    "1. Load base model in fp16 precision\n",
    "2. Convert adapter saved in previous step from fp32 to fp16\n",
    "3. Merge the model\n",
    "4. Run inference both on base model and merged model for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find S3 path for the last job that ran successfully. You can find this from the SageMaker console \n",
    "\n",
    "# *** Get a job name from the AWS console for the last training run or from the above cell\n",
    "job_name = latest_run_job_name\n",
    "\n",
    "def get_s3_path_from_job_name(job_name):\n",
    "    # Create a Boto3 SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # Describe the training job\n",
    "    response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "    \n",
    "    # Extract the model artifacts S3 path\n",
    "    model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Extract the output path (this is the general output location)\n",
    "    output_path = response['OutputDataConfig']['S3OutputPath']\n",
    "    \n",
    "    return model_artifacts_s3_path, output_path\n",
    "\n",
    "\n",
    "model_artifacts, output_path = get_s3_path_from_job_name(job_name)\n",
    "\n",
    "\n",
    "print(f\"Model artifacts S3 path: {model_artifacts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_dir_path=f\"{model_artifacts}/deepseek/output/\"\n",
    "\n",
    "print(f'\\nAdapter S3 Dir path:{adapter_dir_path} \\n')\n",
    "\n",
    "!aws s3 ls {adapter_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3://sagemaker-us-west-2-015476483300/deepseek-8b-finetune-2025-02-04-17-06-44-700/output/model/deepseek/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# Define Training Job Name \n",
    "job_name = f'llama3-1-8b-merge-adapter'\n",
    "\n",
    "pytorch_estimator_adapter = PyTorch(\n",
    "    entry_point= 'merge_model_adapter.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    dependencies=['./rouge'], \n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=5800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    hyperparameters={\n",
    "        \"model_id\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",  # Hugging Face model id\n",
    "        \"hf_token\": \"\",\n",
    "        \"dataset_name\":dataset_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'adapter': adapter_dir_path,\n",
    "  'testdata': test_dataset_s3_path\n",
    "  }\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_estimator_adapter.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
