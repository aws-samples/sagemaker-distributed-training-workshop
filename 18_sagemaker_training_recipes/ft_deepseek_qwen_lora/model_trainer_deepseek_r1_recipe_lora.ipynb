{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Fine-tune DeepSeek-R1-Distill-Qwen-7B using SageMaker Hyperpod recipes and ModelTrainer\n",
    "\n",
    "In this notebook, we fine-tune [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) on Amazon SageMaker AI, using SageMaker Hyperpod recies and [ModelTrainer](https://sagemaker.readthedocs.io/en/v2.239.0/api/training/model_trainer.html) class\n",
    "\n",
    "Recipe: [DeepSeek R1 Distill Qwen 7b - LoRA](https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/deepseek/hf_deepseek_r1_distilled_qwen_7b_seq16k_gpu_lora.yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca016c-d4fa-4213-a7b3-03b449551449",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Our first step is to install Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907944ea-dbfb-4de0-9e13-1fd28c901031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install -r ./scripts/requirements.txt --upgrade\n",
    "%pip install -q -U s3fs boto3 botocore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19163cfd-f05e-47ca-b086-a17163a2269a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb17ac-0844-4f99-bfd6-7f5ea7952b38",
   "metadata": {},
   "source": [
    "## Global variables\n",
    "\n",
    "This section contains python variables used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab776e2-a4a8-41bc-899f-4e46e6b2b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# HuggingFace Model ID\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "\n",
    "# Max number of steps for the training loop\n",
    "max_steps = 215\n",
    "\n",
    "# define Training Job Name \n",
    "job_prefix = f\"train-{model_id.split('/')[-1].replace('.', '-')}-recipe-lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eaeba6-3bed-4b68-a344-ab82926a7ae9",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82089d28-b97a-4956-83fb-d8c46d44fdb5",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "In this example, we use the [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset from Hugging Face. The FreedomIntelligence/medical-o1-reasoning-SFT is used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning. This dataset is constructed using GPT-4o, which searches for solutions to verifiable medical problems and validates them through a medical verifier.\n",
    "\n",
    "For details, see the paper and GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f25c98-0f5b-468d-8257-6efb6af4c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"FreedomIntelligence/medical-o1-reasoning-SFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18126f2e-3599-4cf3-ac74-98e50727af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a medical analysis prompt based on patient information.\n",
    "    \n",
    "    Args:\n",
    "        data_point (dict): Dictionary containing target and meaning_representation keys\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the formatted prompt\n",
    "    \"\"\"\n",
    "    full_prompt = f\"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "    Write a response that appropriately completes the request. \n",
    "    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "    ### Instruction:\n",
    "    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "    Please answer the following medical question. \n",
    "\n",
    "    ### Question:\n",
    "    {data_point[\"Question\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"Complex_CoT\"]}\n",
    "\n",
    "    \"\"\"\n",
    "    return {\"prompt\": full_prompt.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696abde-07c9-4867-ac9c-3e8d16611491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the HF hub\n",
    "train_set = load_dataset(dataset_name, 'en', split=\"train[5%:]\")\n",
    "test_set = load_dataset(dataset_name, 'en', split=\"train[:5%]\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(train_set.features)\n",
    "\n",
    "train_dataset = train_set.map(\n",
    "    generate_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "test_dataset = test_set.map(\n",
    "    generate_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b460fea-7748-4567-9c53-951dff55bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review dataset\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e5b1c-aae4-495f-a6ea-b44f75efa2b7",
   "metadata": {},
   "source": [
    "Load the DeepSeek-R1 Distill Qwen 7B tokenizer from the Hugging Face Transformers library, and generate tokens for the train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206b40fb-e86a-4b5f-bc36-4e348d12fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Model & Tokenizer\n",
    "####################\n",
    "max_seq_length=1024\n",
    "\n",
    "# Initialize a tokenizer by loading a pre-trained tokenizer configuration, using the fast tokenizer implementation if available.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "def tokenize(text):\n",
    "    result = tokenizer(\n",
    "        text['prompt'],\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e401b6-fbdd-4b54-8d13-40f8e8069b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, remove_columns=[\"prompt\"])\n",
    "test_dataset = test_dataset.map(tokenize, remove_columns=[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Upload the tokenized data to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23966212-5399-49a0-9fd1-b8a436de4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'datasets/deepseek-r1-distilled-qwen-7b-recipe-lora'\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test\"\n",
    "\n",
    "train_dataset.save_to_disk(train_dataset_s3_path)\n",
    "test_dataset.save_to_disk(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c237-28bd-474e-9444-94aaea8e6979",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329683c-6662-45d3-b864-9cb575f92599",
   "metadata": {},
   "source": [
    "## Fine-tune model\n",
    "\n",
    "Below ModelTrainer will train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178118a-0f45-4e5f-9bb1-7e5dee146b62",
   "metadata": {},
   "source": [
    "#### Get PyTorch image_uri\n",
    "\n",
    "We are going to use the native PyTorch container image, pre-built for Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5a03c-7660-4729-bf98-67ecb8ffa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.config import load_sagemaker_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaaf81c-e8fb-4e42-a90d-50c2c55047bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = load_sagemaker_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cecfd-e640-4527-99d4-cb3cec9093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\" # Override the instance type if you want to get a different container version\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df7700-7c66-4af8-aea0-da0e5af493bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = (\n",
    "    f\"658645717510.dkr.ecr.{sagemaker_session.boto_session.region_name}.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11af6a-e418-478f-85c0-0df8d0947d3c",
   "metadata": {},
   "source": [
    "Define checkpoint s3 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa1e8b-a37d-4086-aa89-df07f81cda55",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_s3_path = f\"s3://{bucket_name}/deepseek-r1-distilled-qwen-7b-recipe-lora/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cabb4d-b0b2-498c-95cb-41ed7d05ee65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:21.382486Z",
     "start_time": "2023-09-03T00:02:20.962208Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import CheckpointConfig, Compute, InputData, SourceCode, StoppingCondition\n",
    "from sagemaker.modules.distributed import Torchrun\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "\n",
    "instance_count = 1\n",
    "\n",
    "# Working override for custom dataset\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"results_dir\": \"/opt/ml/model\",\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"num_nodes\": instance_count, # Required when instance_count > 1,\n",
    "        \"max_steps\": max_steps,\n",
    "    },\n",
    "    \"exp_manager\": {\n",
    "        \"exp_dir\": \"/opt/ml/output\",\n",
    "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
    "    },\n",
    "    \"use_smp_model\": False, # Required for PEFT\n",
    "    \"model\": {\n",
    "        \"hf_model_name_or_path\": model_id,\n",
    "        \"train_batch_size\": 14,\n",
    "        \"val_batch_size\": 2,\n",
    "        \"data\": {\n",
    "            \"train_dir\": \"/opt/ml/input/data/train\",\n",
    "            \"val_dir\": \"/opt/ml/input/data/test\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=instance_type,\n",
    "    instance_count=instance_count,\n",
    "    keep_alive_period_in_seconds=1800\n",
    ")\n",
    "\n",
    "model_trainer = ModelTrainer.from_recipe(\n",
    "    training_image=image_uri,\n",
    "    training_recipe=\"fine-tuning/deepseek/hf_deepseek_r1_distilled_qwen_7b_seq8k_gpu_lora\",\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    requirements=\"./scripts/requirements.txt\",\n",
    "    base_job_name=job_prefix,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(\n",
    "        max_runtime_in_seconds=7200\n",
    "    ),\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        s3_uri=f\"{checkpoint_s3_path}/{job_prefix}\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a386bd9-172c-485c-af45-ebc1d126470b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"train\",\n",
    "    data_source=train_dataset_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "test_input = InputData(\n",
    "    channel_name=\"test\",\n",
    "    data_source=test_dataset_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "data = [train_input, test_input]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e13aa-1df2-43fc-bae4-15f5b7113191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "model_trainer.train(input_data_config=data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17440d46-358f-43af-8a5c-34bac47625d2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6c40ab-940c-4158-9147-b77a825a09da",
   "metadata": {},
   "source": [
    "Define S3 path for the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e0edc-88f7-4d53-a5fa-344107a5a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = f\"s3://{bucket_name}/deepseek-r1-distilled-qwen-7b-recipe-lora/checkpoints/{job_prefix}\"\n",
    "\n",
    "trained_model=f\"{checkpoint_dir}/peft_full/steps_{max_steps}/final-model/\"\n",
    "\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ce8a4-348f-457e-a360-543352ca855b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {trained_model}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d403f8b-fabc-4674-a51e-8bb2794e8101",
   "metadata": {},
   "source": [
    "### Run evaluation job using SageMaker ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7feb15b-30dc-4254-ac48-87c8cf0e4cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p4d.24xlarge\" # Override the instance type if you want to get a different container version\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd5626-8fdc-4129-aa29-5a442686a78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sagemaker_session.boto_session.region_name,\n",
    "    version=\"2.4\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9779ddf6-ac99-4875-b592-15d218f88116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import Compute, InputData, OutputDataConfig, SourceCode, StoppingCondition\n",
    "from sagemaker.modules.distributed import Torchrun\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "\n",
    "# Define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./scripts\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    entry_script=\"evaluate_recipe.py\",\n",
    "    \n",
    ")\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    keep_alive_period_in_seconds=1800\n",
    ")\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f\"eval-{job_prefix}\"\n",
    "\n",
    "# Define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    stopping_condition=StoppingCondition(\n",
    "        max_runtime_in_seconds=7200\n",
    "    ),\n",
    "    hyperparameters={\n",
    "        \"model_id\": model_id,  # Hugging Face model id\n",
    "        \"dataset_name\": dataset_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4d884-be75-409a-8e6d-9be6fbf186a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"adapterdir\",\n",
    "    data_source=trained_model,\n",
    ")\n",
    "\n",
    "test_input = InputData(\n",
    "    channel_name=\"testdata\",\n",
    "    data_source=test_dataset_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "data = [train_input, test_input]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b210-c40c-4680-a23f-8d91037dbcf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "model_trainer.train(input_data_config=data, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4a8df-7509-458e-8a1d-0975b2934d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
