{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Fine-tune DeepSeek-R1-Distill-Qwen-7B using SageMaker Hyperpod recipes and ModelTrainer\n",
    "\n",
    "In this notebook, we fine-tune [deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) on Amazon SageMaker AI, using SageMaker Hyperpod recies and [ModelTrainer](https://sagemaker.readthedocs.io/en/v2.239.0/api/training/model_trainer.html) class\n",
    "\n",
    "Recipe: [DeepSeek R1 Distill Qwen 7b - LoRA](https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/deepseek/hf_deepseek_r1_distilled_qwen_7b_seq16k_gpu_lora.yaml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca016c-d4fa-4213-a7b3-03b449551449",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Our first step is to install Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8eb4a-ab28-4f8a-8a5a-ba42c1752382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"setuptools\" \"sagemaker==2.239.1\" \"graphene\" \"datasets==3.2.0\" \"transformers==4.44.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b01f58-df1e-4f86-8647-98539bac3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.modules import Session\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from sagemaker.modules.configs import Compute\n",
    "from sagemaker.modules.configs import Networking\n",
    "from sagemaker.modules.configs import FileSystemDataSource\n",
    "from sagemaker.modules.configs import S3DataSource\n",
    "from sagemaker.modules.configs import InputData\n",
    "from sagemaker.modules.configs import StoppingCondition\n",
    "from sagemaker.modules.configs import SourceCode\n",
    "from typing import Any\n",
    "from utility import *\n",
    "import sagemaker\n",
    "import boto3\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997649-d05e-42a7-bfc1-f9dc0596304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "# HuggingFace Model ID\n",
    "model_id = \"deepseek-ai/DeepSeek-R1\"\n",
    "\n",
    "# VPC config\n",
    "network_config={\n",
    "   \"subnets\": [\"subnet-xxxx\"], # e.g. ['subnet-xxxx','subnet-yyyyy']\n",
    "   \"security_group_ids\": [\"sg-xxxx\"] # e.g. [\"sg-xxxx\"]\n",
    "}\n",
    "\n",
    "# FSx mount name\n",
    "fsx_mount_point='/xxxx'\n",
    "\n",
    "# HuggingFace token\n",
    "hf_token=\"<>\"\n",
    "\n",
    "fsx_dir_basemodel=\"deepseek_r1_671b_tj\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c103a41d-93f9-4d89-a44e-ee188212a83a",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "In this example, we use the [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset from Hugging Face. The FreedomIntelligence/medical-o1-reasoning-SFT is used to fine-tune HuatuoGPT-o1, a medical LLM designed for advanced medical reasoning. This dataset is constructed using GPT-4o, which searches for solutions to verifiable medical problems and validates them through a medical verifier.\n",
    "\n",
    "For details, see the paper and GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94f80c-26a3-4ba4-bf1a-05f96461a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"FreedomIntelligence/medical-o1-reasoning-SFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0dff7a-86d7-419b-bf4f-b7123da214b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generates a medical analysis prompt based on patient information.\n",
    "    \n",
    "    Args:\n",
    "        data_point (dict): Dictionary containing target and meaning_representation keys\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the formatted prompt\n",
    "    \"\"\"\n",
    "    full_prompt = f\"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "    Write a response that appropriately completes the request. \n",
    "    Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "    ### Instruction:\n",
    "    You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \n",
    "    Please answer the following medical question. \n",
    "\n",
    "    ### Question:\n",
    "    {data_point[\"Question\"]}\n",
    "\n",
    "    ### Response:\n",
    "    {data_point[\"Complex_CoT\"]}\n",
    "\n",
    "    \"\"\"\n",
    "    return {\"prompt\": full_prompt.strip()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f200c10-a4c3-414a-b349-1c0c8864e599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the HF hub\n",
    "train_set = load_dataset(dataset_name, 'en', split=\"train[5%:]\")\n",
    "test_set = load_dataset(dataset_name, 'en', split=\"train[:5%]\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(train_set.features)\n",
    "\n",
    "train_dataset = train_set.map(\n",
    "    generate_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "test_dataset = test_set.map(\n",
    "    generate_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada0102-bfcd-4dcf-9df0-3cbb911c3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review dataset\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8a7f7-2c25-415d-b529-db7f751934ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Model & Tokenizer\n",
    "####################\n",
    "max_seq_length=1024\n",
    "\n",
    "# Initialize a tokenizer by loading a pre-trained tokenizer configuration, using the fast tokenizer implementation if available.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_id,\n",
    "        use_fast=True\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "def tokenize(text):\n",
    "    result = tokenizer(\n",
    "        text['prompt'],\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb6ef3-f06c-497d-89ba-b5dc2fdef8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(tokenize, remove_columns=[\"prompt\"])\n",
    "test_dataset = test_dataset.map(tokenize, remove_columns=[\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c06445-1af0-4b35-9424-50469f272b6d",
   "metadata": {},
   "source": [
    "### Upload the tokenized data to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbff04-04c3-4963-a880-3164d88292c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'datasets/deepseek-r1-distilled-qwen-7b-recipe-lora'\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test\"\n",
    "\n",
    "train_dataset.save_to_disk(train_dataset_s3_path)\n",
    "test_dataset.save_to_disk(test_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41cbba-33e7-40b5-a740-0409f6ee9b3e",
   "metadata": {},
   "source": [
    "# Common functions \n",
    "\n",
    "Let us define some utility function to run model training using the SageMaker ModelTrainer class.\n",
    "\n",
    "For additional information about ModelTrainer, you can refer to Accelerate your ML lifecycle using the new and improved Amazon SageMaker Python SDK â€“ Part 1: ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1071fc0-0ddb-45e6-8d86-25aaa95de764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_trainer(\n",
    "    use_recipes: bool,\n",
    "    compute: dict,\n",
    "    network: dict,\n",
    "    data_channel: dict,\n",
    "    action: str,\n",
    "    hyperparameters: dict ={},\n",
    "    source_code: str=None,\n",
    "    training_recipe: str=None,\n",
    "    recipe_overrides: str=None,\n",
    "    image_uri: str=None\n",
    ") -> ModelTrainer:\n",
    "    \"\"\"\n",
    "    Creates and executes a model training job using SageMaker.\n",
    "    \n",
    "    Args:\n",
    "        use_recipes (bool): Flag to determine if using SageMaker recipes\n",
    "        compute (dict): Compute configuration for training\n",
    "        source_code (str): Path to source code\n",
    "        network (dict): Network configuration\n",
    "        data_channel (dict): Data channel configuration\n",
    "        action (str): Action identifier for job naming\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If required parameters are missing or invalid\n",
    "    \"\"\"\n",
    "    # Parameter validation\n",
    "    required_params = {\n",
    "        'use_recipes': use_recipes,\n",
    "        'compute': compute,\n",
    "        **({'source_code': source_code} if source_code is not None else {}),\n",
    "        'network': network,\n",
    "        'data_channel': data_channel,\n",
    "        'action': action,\n",
    "        **({'training_recipe': training_recipe} if training_recipe is not None else {}),\n",
    "        **({'recipe_overrides': recipe_overrides} if recipe_overrides is not None else {}),\n",
    "    }\n",
    "    \n",
    "    for param_name, param_value in required_params.items():\n",
    "        if param_value is None:\n",
    "            raise ValueError(f\"Required parameter '{param_name}' is missing\")\n",
    "            \n",
    "\n",
    "    # Job name creation\n",
    "    job_name = f'model-trainer-deepseek-{action}'\n",
    "\n",
    "    # Image URI selection\n",
    "    if image_uri is None or len(image_uri) == 0:\n",
    "        if use_recipes:\n",
    "            image_uri = (\n",
    "                \"658645717510.dkr.ecr.us-east-1.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\"\n",
    "            )\n",
    "        else:\n",
    "            image_uri = sagemaker.image_uris.retrieve(\n",
    "                framework=\"pytorch\",\n",
    "                region=sagemaker_session.boto_session.region_name,\n",
    "                version=\"2.4\",\n",
    "                instance_type=compute.instance_type,\n",
    "                image_scope=\"training\"\n",
    "            )\n",
    "\n",
    "    # Setting up stopping condition\n",
    "    stopping_condition = StoppingCondition(max_runtime_in_seconds=43200)\n",
    "\n",
    "    # Estimator configuration\n",
    "    estimator_config = {\n",
    "        'training_image': image_uri,\n",
    "        'source_code': source_code,\n",
    "        'networking': network,\n",
    "        'compute': compute,\n",
    "        'base_job_name': job_name,\n",
    "        'stopping_condition': stopping_condition\n",
    "    }\n",
    "\n",
    "    if(len(hyperparameters) != 0):\n",
    "        estimator_config.update({'hyperparameters':hyperparameters})\n",
    "\n",
    "    if(source_code is None):\n",
    "        estimator_config.pop('source_code')\n",
    "\n",
    "    # Create and execute model trainer\n",
    "    try:\n",
    "        if(use_recipes):\n",
    "            estimator_config.update({'training_recipe':training_recipe})\n",
    "            estimator_config.update({'recipe_overrides':recipe_overrides})\n",
    "            estimator_config.update({'requirements':\"scripts/requirements.txt\"})\n",
    "    \n",
    "            print(f'estimator_config:{estimator_config}')\n",
    "            model_trainer= ModelTrainer.from_recipe(**estimator_config) \n",
    "        else: \n",
    "            print(f'estimator_config:{estimator_config}')\n",
    "            model_trainer= ModelTrainer(**estimator_config)\n",
    "        \n",
    "        return model_trainer\n",
    "        #model_trainer.fit(input_data_config=[data_channel], wait=True)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to create or execute model trainer: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e0c8e-cdd0-4dd3-9106-2b8fe80482af",
   "metadata": {},
   "source": [
    "# STEP 1: Download model to Amazon FSx for Lustre directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adce88a0-35a4-4b7a-a9e2-8572e0759a31",
   "metadata": {},
   "source": [
    "In this step, we will download the DeepSeek-R1 model to FSx directory.\n",
    "\n",
    "Select the instance type, FSx data channel, network configuration for the training job, source code and define the ModelTrainer class to run the training job on ml.c5.18xlarge instance to download DeepSeek-R1 model from huggingface hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa29e6b-4817-4942-b4c0-60e44fa1c0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute instance\n",
    "compute = ComputeCreator.create(\n",
    "    instance_type=\"ml.c5.18xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "# Create FSx data channel\n",
    "data_channel = FSxDataChannelCreator.create_channel(\n",
    "    directory_path=fsx_mount_point\n",
    ")\n",
    "\n",
    "# Create network configuration\n",
    "network = NetworkConfigCreator.create_network_config(network_config)\n",
    "\n",
    "# Set up source code configuration\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_script=\"download.py\"\n",
    ")\n",
    "\n",
    "# Print configuration details\n",
    "print(f'Compute Instance created: {compute}')\n",
    "print(f'Type: {type(compute)}\\n')\n",
    "\n",
    "print(f'Data Channel created: {data_channel}')\n",
    "print(f'Type: {type(data_channel)}\\n')\n",
    "\n",
    "print(f'Network created: {network}')\n",
    "print(f'Type: {type(network)}\\n')\n",
    "\n",
    "print(f'Source code created: {source_code}')\n",
    "print(f'Type: {type(source_code)}\\n')\n",
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,      # Hugging Face model id\n",
    "    \"hf_token\": hf_token,\n",
    "    \"local_fsx_dir\": fsx_dir_basemodel\n",
    "}\n",
    "\n",
    "# Create model trainer\n",
    "model_trainer = create_model_trainer(\n",
    "    use_recipes=False,\n",
    "    compute=compute,\n",
    "    network=network,\n",
    "    data_channel=data_channel,          \n",
    "    action=\"download\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_code=source_code\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9bfb9-5247-4227-8646-8e49cb4ae192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_trainer.train(input_data_config=[data_channel], wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0620499-2abe-4876-a7b1-73e48c89be59",
   "metadata": {},
   "source": [
    "# STEP 2: Convert DeepSeek R1 from FP8 to BF16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789fc686-30fa-47d1-9083-0751e24be1ef",
   "metadata": {},
   "source": [
    "HyperPod recipes disable FP8 in the QLoRA and LoRA recipes. BF16 is the most optimal precision type for generalizing PEFT training configurations to various datasets. That being said, the default weights provided by the DeepSeek team on their official R1 repository are of type FP8. To ensure stable fine-tuning for a DeepSeek-R1 model, we will first convert it to BF16 using the fp8_cast_bf16.py command-line script provided by DeepSeek. Executing this script, will copy over the converted BF16 weights in safetensor format to the specified output directory.\n",
    "\n",
    "We will use ModelTrainer class to execute the conversion using training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a5425-e252-483a-bae5-5346aca5662f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define constants\n",
    "fsx_modeldir_bf16 = \"deepseek_r1_bf16\"\n",
    "FSX_DIR_PATH = f\"{fsx_mount_point}/{fsx_dir_basemodel}\"\n",
    "\n",
    "# Create compute instance\n",
    "compute = ComputeCreator.create(\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "# Create FSx data channel\n",
    "data_channel = FSxDataChannelCreator.create_channel(\n",
    "    directory_path=FSX_DIR_PATH\n",
    ")\n",
    "\n",
    "# Create network configuration\n",
    "network = NetworkConfigCreator.create_network_config(network_config)\n",
    "\n",
    "# Set up source code configuration\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_script=\"convert.sh\"\n",
    ")\n",
    "\n",
    "# Print configuration details\n",
    "print(f'Compute Instance created: {compute}')\n",
    "print(f'Type: {type(compute)}\\n')\n",
    "\n",
    "print(f'Data Channel created: {data_channel}')\n",
    "print(f'Type: {type(data_channel)}\\n')\n",
    "\n",
    "print(f'Network created: {network}')\n",
    "print(f'Type: {type(network)}\\n')\n",
    "\n",
    "print(f'Source code created: {source_code}')\n",
    "print(f'Type: {type(source_code)}\\n')\n",
    "\n",
    "# Define hyperparameters for model conversion\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,          # Hugging Face model id\n",
    "    \"hf_token\": hf_token,\n",
    "    \"converted_fsx_dir\": fsx_modeldir_bf16\n",
    "}\n",
    "\n",
    "# Create model trainer for conversion\n",
    "model_trainer = create_model_trainer(\n",
    "    use_recipes=False,\n",
    "    compute=compute,\n",
    "    network=network,\n",
    "    data_channel=data_channel,          \n",
    "    action=\"convert\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_code=source_code\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d433e769-171d-491a-8e39-5fe3a2b5ab96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_trainer.train(input_data_config=[data_channel], wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e39be14-676a-4121-ab0b-2e6a7adbac26",
   "metadata": {},
   "source": [
    "# STEP 3: Fine-tune the DeepSeek-R1 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289079d-b40f-4eac-b4d1-333e4007ec3f",
   "metadata": {},
   "source": [
    "Our next phase involves the fine-tuning of the DeepSeek-R1 model utilizing two ml.p5.48xlarge instances, leveraging distributed training. We'll implement this through SageMaker's recipe \"hf_deepseek_r1_671b_seq8k_gpu_qlora\", which incorporates the Quantized Low-Rank Adaptation (QLoRA) methodology. QLoRA makes LLM trainable on limited compute by quantizing the base model to 4-bit precision while using small, trainable low-rank adapters for fine-tuning, dramatically reducing memory requirements without sacrificing model quality.\n",
    "\n",
    "We can override recipe parameters, to tune the script to our needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd9e36-b3fb-4ee3-b448-bef135b01931",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"results_dir\": \"/opt/ml/model\",\n",
    "    },\n",
    "    \"exp_manager\": {\n",
    "        \"exp_dir\": \"/opt/ml/input/data/modelweights/output/\",\n",
    "        \"explicit_log_dir\": \"/opt/ml/output/tensorboard\",\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"hf_model_name_or_path\": \"/opt/ml/input/data/modelweights/\",\n",
    "        \"data\": {\n",
    "            #\"use_synthetic_data\": True,\n",
    "            \"train_dir\": \"/opt/ml/input/data/train\",\n",
    "            \"val_dir\": \"/opt/ml/input/data/test\",\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4903b-8565-4859-acd6-3aae1ce3c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compute configuration with P5 instances\n",
    "compute = ComputeCreator.create(\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    instance_count=2\n",
    ")\n",
    "\n",
    "# Construct FSx directory path for model\n",
    "fsx_dir_path = f\"{fsx_mount_point}/{fsx_dir_basemodel}/{fsx_modeldir_bf16}\"\n",
    "\n",
    "# Set up network configuration\n",
    "network = NetworkConfigCreator.create_network_config(network_config)\n",
    "\n",
    "# Print configuration details\n",
    "print(f'Compute Instance created: {compute}')\n",
    "print(f'Type: {type(compute)}\\n')\n",
    "\n",
    "print(f'Data Channel created: {data_channel}')\n",
    "print(f'Type: {type(data_channel)}\\n')\n",
    "\n",
    "print(f'Network created: {network}')\n",
    "print(f'Type: {type(network)}\\n')\n",
    "\n",
    "# Create model trainer for fine-tuning\n",
    "model_trainer = create_model_trainer(\n",
    "    use_recipes=True,\n",
    "    compute=compute,\n",
    "    network=network,\n",
    "    data_channel=data_channel,          \n",
    "    action=\"finetune\",\n",
    "    training_recipe='fine-tuning/deepseek/hf_deepseek_r1_671b_seq8k_gpu_qlora',\n",
    "    recipe_overrides=recipe_overrides                       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c0595-c3c9-4609-b5d2-0cf595b9b0c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData\n",
    "\n",
    "# Create FSx data channel\n",
    "data_channel = FSxDataChannelCreator.create_channel(\n",
    "    directory_path=fsx_dir_path\n",
    ")\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"train\",\n",
    "    data_source=train_dataset_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "test_input = InputData(\n",
    "    channel_name=\"test\",\n",
    "    data_source=test_dataset_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "data_channel, train_input, test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c4887-9577-48cf-9d0e-8a73ae78d4cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_trainer.train(input_data_config=[data_channel, train_input, test_input], wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa21bd0-6c98-404d-8bcd-2eb1e9104f9c",
   "metadata": {},
   "source": [
    "# STEP 4: Merge the trained adapter with the base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4cb4c-96bb-4351-bb82-7bb04b1df291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Configuration\n",
    "ECR_IMAGE_URI = \"658645717510.dkr.ecr.us-east-1.amazonaws.com/smdistributed-modelparallel:2.4.1-gpu-py311-cu121\"\n",
    "\n",
    "# Create compute configuration with P5 instance\n",
    "compute = ComputeCreator.create(\n",
    "    instance_type=\"ml.p5.48xlarge\",\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "# Construct FSx directory path for model weights\n",
    "fsx_dir_path = f\"{fsx_mount_point}/{fsx_dir_basemodel}/{fsx_modeldir_bf16}\"\n",
    "\n",
    "# Create FSx data channel for model access\n",
    "data_channel = FSxDataChannelCreator.create_channel(\n",
    "    directory_path=fsx_dir_path\n",
    ")\n",
    "\n",
    "# Set up network configuration\n",
    "network = NetworkConfigCreator.create_network_config(network_config)\n",
    "\n",
    "# Print configuration details for verification\n",
    "print(f'Compute Instance created: {compute}')\n",
    "print(f'Type: {type(compute)}\\n')\n",
    "\n",
    "print(f'Data Channel created: {data_channel}')\n",
    "print(f'Type: {type(data_channel)}\\n')\n",
    "\n",
    "print(f'Network created: {network}')\n",
    "print(f'Type: {type(network)}\\n')\n",
    "\n",
    "# Define hyperparameters for model parallel training\n",
    "hyperparameters = {\n",
    "    \"mp_parameters\": {\n",
    "        #\"tensor_parallel_degree\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# Configure source code location and entry point\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_script=\"cli-inference.sh\"\n",
    ")\n",
    "\n",
    "# Create model trainer for adapter merging\n",
    "model_trainer = create_model_trainer(\n",
    "    use_recipes=False,\n",
    "    compute=compute,\n",
    "    network=network,\n",
    "    data_channel=data_channel,          \n",
    "    action=\"mergeadapter\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_code=source_code,\n",
    "    image_uri=ECR_IMAGE_URI\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be7ebc-a881-4abf-972b-0d78ef1a3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(input_data_config=[data_channel], wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
