{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Supervised Fine-Tuning (SFT) of Amazon Nova using Amazon SageMaker Training Job\n",
    "\n",
    "In this notebook, we fine-tune LLM on Amazon SageMaker AI, using Python scripts and SageMaker ModelTrainer for executing a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907944ea-dbfb-4de0-9e13-1fd28c901031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -r ./requirements.txt --upgrade\n",
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c9e5c-c57c-42cd-baf4-e139422cc147",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b6105-ecec-4213-b56d-589238844dca",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce51663-0171-4d54-b16e-f85e3cadb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "bucket_name = sess.default_bucket()\n",
    "default_prefix = sess.default_bucket_prefix\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95b61-8666-4015-bf2e-fcf68ce38c5b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82089d28-b97a-4956-83fb-d8c46d44fdb5",
   "metadata": {},
   "source": [
    "## Prepare the dataset\n",
    "\n",
    "In this example, we are going to load [glaiveai/glaive-function-calling-v2](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2) dataset, an open-source dataset and model suite focused on enabling and improving function calling capabilities for large language models (LLMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481791d-9c86-4d32-a39a-918aff5e432f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glaiveai/glaive-function-calling-v2\", split=\"train[:10000]\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import glaive_to_standard_format\n",
    "\n",
    "processed_dataset = glaive_to_standard_format(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e78ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(processed_dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df908a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.01, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(test))\n",
    "print(\"Number of val elements: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368c020-e9a3-48b3-a53b-45404bba9482",
   "metadata": {},
   "source": [
    "Let's format the dataset by using the prompt style for Amazon Nova:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": [{\"text\": Content of the System prompt}],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\"text\": Content of the user prompt]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\"text\": Content of the answer]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce4f50",
   "metadata": {},
   "source": [
    "The notebook defines utility functions to clean the dataset content by removing prefixes and handling special cases:\n",
    "\n",
    "```python\n",
    "def clean_prefix(content):\n",
    "    # Removes prefixes like \"USER:\", \"ASSISTANT:\", etc.\n",
    "    ...\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "\n",
    "def clean_numbered_conversation(message_list):\n",
    "    # Cleans message lists from None values and converts to proper format\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e55ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_prefix(content):\n",
    "    \"\"\"Remove prefixes from content, according to Nova data_validator\"\"\"\n",
    "    prefixes = [\n",
    "        \"SYSTEM:\",\n",
    "        \"System:\",\n",
    "        \"USER:\",\n",
    "        \"User:\",\n",
    "        \"ASSISTANT:\",\n",
    "        \"Assistant:\",\n",
    "        \"Bot:\",\n",
    "        \"BOT:\",\n",
    "    ]\n",
    "\n",
    "    # Handle array case (list of content items)\n",
    "    if hasattr(content, \"__iter__\") and not isinstance(content, str):\n",
    "        for i, item in enumerate(content):\n",
    "            if isinstance(item, dict) and \"text\" in item:\n",
    "                text = item[\"text\"]\n",
    "                if isinstance(text, str):\n",
    "                    # Clean line by line for multi-line text\n",
    "                    lines = text.split(\"\\n\")\n",
    "                    cleaned_lines = []\n",
    "                    for line in lines:\n",
    "                        cleaned_line = line.strip()\n",
    "                        for prefix in prefixes:\n",
    "                            if cleaned_line.startswith(prefix):\n",
    "                                cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                                break\n",
    "                        cleaned_lines.append(cleaned_line)\n",
    "                    item[\"text\"] = \"\\n\".join(cleaned_lines)\n",
    "        return content\n",
    "\n",
    "    # Handle string case\n",
    "    if isinstance(content, str):\n",
    "        lines = content.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            cleaned_line = line.strip()\n",
    "            for prefix in prefixes:\n",
    "                if cleaned_line.startswith(prefix):\n",
    "                    cleaned_line = cleaned_line[len(prefix) :].strip()\n",
    "                    break\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return content\n",
    "\n",
    "\n",
    "def clean_message_list(message_list):\n",
    "    \"\"\"Clean message list from None values and convert to list of dicts if needed.\"\"\"\n",
    "    if isinstance(message_list, str):\n",
    "        message_list = json.loads(message_list)\n",
    "\n",
    "    tmp_cleaned = []\n",
    "    for msg in message_list:\n",
    "        new_msg = {}\n",
    "        for key, value in msg.items():\n",
    "            if key in [\"content\"]:\n",
    "                if value is None or str(value).lower() == \"None\":\n",
    "                    continue\n",
    "            new_msg[key] = value\n",
    "        tmp_cleaned.append(new_msg)\n",
    "\n",
    "    cleaned = []\n",
    "    for item in tmp_cleaned:\n",
    "        content = item[\"content\"]\n",
    "        for content_item in content:\n",
    "            if isinstance(content_item, dict) and \"text\" in content_item:\n",
    "                text = clean_numbered_conversation(content_item[\"text\"])\n",
    "                content_item[\"text\"] = clean_prefix(text)\n",
    "        cleaned.append({\"role\": item[\"role\"], \"content\": content})\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Additional function to specifically handle the numbered conversation format\n",
    "def clean_numbered_conversation(text):\n",
    "    \"\"\"Clean numbered conversation format like '1. User: ...'\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Pattern to match numbered items with User: or Assistant: prefixes\n",
    "    pattern = r\"(\\d+\\.\\s*)(User:|Assistant:)\\s*\"\n",
    "\n",
    "    # Replace the pattern, keeping the number but removing the role prefix\n",
    "    cleaned_text = re.sub(pattern, r\"\\1\", text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8de43",
   "metadata": {},
   "source": [
    "These functions transform the dataset into the format required by Nova models, handling tool calls and formatting:\n",
    "\n",
    "```python\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    # Transforms tool format to Nova's expected format\n",
    "    ...\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    # Prepares dataset in the required format for Nova models\n",
    "    ...\n",
    "\n",
    "def prepare_dataset_test(sample):\n",
    "    # Formats validation dataset for evaluation\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb8edd-35c0-4cf1-82d3-54417bdabd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:01.435195Z",
     "start_time": "2023-09-03T00:02:01.429794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def transform_tool_format(tool):\n",
    "    \"\"\"Transform tool from old format to Nova format.\"\"\"\n",
    "    if \"function\" not in tool:\n",
    "        return tool\n",
    "\n",
    "    function = tool[\"function\"]\n",
    "    return {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": function[\"name\"],\n",
    "            \"description\": function[\"description\"],\n",
    "            \"inputSchema\": {\"json\": function[\"parameters\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    \"\"\"Prepare dataset in the required format for Nova models\"\"\"\n",
    "    messages = {\"system\": [], \"messages\": []}\n",
    "\n",
    "    # Process tools upfront if they exist\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    formatted_text = (\n",
    "        \"\"  # Initialize outside the loop to avoid undefined variable issues\n",
    "    )\n",
    "\n",
    "    for message in sample[\"messages\"]:\n",
    "        role = message[\"role\"]\n",
    "\n",
    "        if role == \"system\" and tools:\n",
    "            # Build system message with tools\n",
    "            system_text = (\n",
    "                f\"{message['content']}\\n\"\n",
    "                \"You may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                \"<tools>\\n\"\n",
    "                f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                \"</tools>\\n\\n\"\n",
    "                \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "            )\n",
    "            messages[\"system\"] = [{\"text\": system_text.lower()}]\n",
    "\n",
    "        elif role == \"user\":\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": message[\"content\"].lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"tool\":\n",
    "            formatted_text += message[\"content\"]\n",
    "            messages[\"messages\"].append(\n",
    "                {\"role\": \"user\", \"content\": [{\"text\": formatted_text.lower()}]}\n",
    "            )\n",
    "\n",
    "        elif role == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Process tool calls\n",
    "                tool_calls_text = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    tool_call_json = {\n",
    "                        \"name\": function_data[\"name\"],\n",
    "                        \"parameters\": arguments,\n",
    "                    }\n",
    "                    tool_calls_text.append(json.dumps(tool_call_json))\n",
    "\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"text\": \"\".join(tool_calls_text).lower()}],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": [{\"text\": message[\"content\"].lower()}],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Remove the last message if it's not from assistant\n",
    "    if messages[\"messages\"] and messages[\"messages\"][-1][\"role\"] != \"assistant\":\n",
    "        messages[\"messages\"].pop()\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dd482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_validation(sample):\n",
    "    \"\"\"Parse sample and format it for validation dataset.\"\"\"\n",
    "    # Process tools\n",
    "    tools = json.loads(sample[\"tools\"]) if sample.get(\"tools\") else []\n",
    "    transformed_tools = [transform_tool_format(tool) for tool in tools]\n",
    "\n",
    "    # Initialize result\n",
    "    result = []\n",
    "    conversation_history = []\n",
    "\n",
    "    # Extract system message\n",
    "    system_content = \"\"\n",
    "    for message in sample[\"messages\"]:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            system_content = message[\"content\"]\n",
    "            if tools:\n",
    "                system_content += (\n",
    "                    \"\\nYou may call one or more functions to assist with the user query.\\n\\n\"\n",
    "                    \"You are provided with function signatures within <tools></tools> XML tags:\\n\"\n",
    "                    \"<tools>\\n\"\n",
    "                    f\"{json.dumps({'tools': transformed_tools})}\\n\"\n",
    "                    \"</tools>\\n\\n\"\n",
    "                    \"For each function call, return a json object with function name and parameters:\\n\"\n",
    "                    '{\"name\": function name, \"parameters\": dictionary of argument name and its value}'\n",
    "                )\n",
    "            break\n",
    "\n",
    "    # Process conversation turns\n",
    "    for i, message in enumerate(sample[\"messages\"]):\n",
    "        if message[\"role\"] == \"system\":\n",
    "            continue\n",
    "\n",
    "        # Add message to conversation history\n",
    "        if message[\"role\"] == \"user\":\n",
    "            conversation_history.append(f\"##User: {message['content']}\")\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "\n",
    "                conversation_history.append(f\"##Assistant: {target}\")\n",
    "            else:\n",
    "                conversation_history.append(f\"##Assistant: {message['content']}\")\n",
    "        elif message[\"role\"] == \"tool\":\n",
    "            conversation_history.append(f\"## Function: {message['content']}\")\n",
    "\n",
    "        # Create input-target pair when we have an assistant message\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # Input is system message + all previous conversation\n",
    "            input_text = \"\\n\".join(conversation_history[:-1])\n",
    "\n",
    "            # Target is the assistant's response\n",
    "            if message.get(\"tool_calls\"):\n",
    "                # Format tool calls\n",
    "                target_parts = []\n",
    "                for tool_call in message[\"tool_calls\"]:\n",
    "                    function_data = tool_call[\"function\"]\n",
    "                    arguments = (\n",
    "                        json.loads(function_data[\"arguments\"])\n",
    "                        if isinstance(function_data[\"arguments\"], str)\n",
    "                        else function_data[\"arguments\"]\n",
    "                    )\n",
    "                    target_parts.append(\n",
    "                        json.dumps(\n",
    "                            {\"name\": function_data[\"name\"], \"parameters\": arguments}\n",
    "                        )\n",
    "                    )\n",
    "                target = \"\".join(target_parts)\n",
    "            else:\n",
    "                target = message[\"content\"]\n",
    "\n",
    "            result.append(\n",
    "                {\n",
    "                    \"system\": system_content.lower(),\n",
    "                    \"query\": input_text.lower(),\n",
    "                    \"response\": target.lower(),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cbedd-7403-467e-8cc6-1d2550d8b8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:10.364459Z",
     "start_time": "2023-09-03T00:02:09.672705Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\"train\": train_dataset, \"test\": test_dataset, \"val\": val_dataset}\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=train_dataset.features\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.to_pandas()\n",
    "\n",
    "train_dataset[\"messages\"] = train_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "print(train_dataset.iloc[randint(0, len(train_dataset))].to_json())\n",
    "\n",
    "test_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset, remove_columns=test_dataset.features\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.to_pandas()\n",
    "\n",
    "test_dataset[\"messages\"] = test_dataset[\"messages\"].apply(clean_message_list)\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(\n",
    "    prepare_dataset_validation, remove_columns=val_dataset.features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7f8ba",
   "metadata": {},
   "source": [
    "The validation dataset will be formatted with the structure below:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"system\": \"Optional - String containing the system prompt, that sets the behavior, role, or personality of the model\",\n",
    "    \"query\": \"String containing the input prompt\",\n",
    "    \"response\": \"String containing the expected model output\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Flatten the dataset\n",
    "all_examples = []\n",
    "for examples_list in val_dataset:\n",
    "    # The first column contains the list of examples\n",
    "    column_name = val_dataset.column_names[0]\n",
    "    examples = examples_list[column_name]\n",
    "    all_examples.extend(examples)\n",
    "\n",
    "# Create a new dataset with the desired structure\n",
    "val_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"system\": [example[\"system\"] for example in all_examples],\n",
    "        \"query\": [example[\"query\"] for example in all_examples],\n",
    "        \"response\": [example[\"response\"] for example in all_examples],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(val_dataset[randint(0, len(val_dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f29e5-4aed-4939-8d51-ad3c5268299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db05863-3acb-483b-8e34-2aacbdbc68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-sft\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-sft\"\n",
    "\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.jsonl\"\n",
    "test_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/test/dataset.jsonl\"\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/gen_qa.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0321-1bd5-4c62-845a-bb1b9a3891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "os.makedirs(\"./data/train\", exist_ok=True)\n",
    "os.makedirs(\"./data/test\", exist_ok=True)\n",
    "\n",
    "train_dataset.to_json(\"./data/train/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "test_dataset.to_json(\"./data/test/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "val_dataset.to_json(\"./data/val/gen_qa.jsonl\")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/test/dataset.jsonl\", bucket_name, f\"{input_path}/test/dataset.jsonl\"\n",
    ")\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/val/gen_qa.jsonl\", bucket_name, f\"{input_path}/val/gen_qa.jsonl\"\n",
    ")\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c237-28bd-474e-9444-94aaea8e6979",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457beda-117d-4782-9f04-0680c199e98a",
   "metadata": {},
   "source": [
    "## Model fine-tuning\n",
    "\n",
    "We now define the PyTorch estimator to run the a Supervised fine-tuning (SFT) workload on the formatted tool-calling dataset for Amazon Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cecfd-e640-4527-99d4-cb3cec9093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 4\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bc99b",
   "metadata": {},
   "source": [
    "Let's define the container to execute the SFT workload for Amazon Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df7700-7c66-4af8-aea0-da0e5af493bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-fine-tune-repo:SM-TJ-SFT-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d100cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nova-lite/prod\"\n",
    "recipe = \"fine-tuning/nova/nova_lite_p5_gpu_sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95841fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-sft\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,\n",
    "    },\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a386bd9-172c-485c-af45-ebc1d126470b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "train_input = TrainingInput(\n",
    "    s3_data=train_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")\n",
    "\n",
    "test_input = TrainingInput(\n",
    "    s3_data=test_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"Converse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e13aa-1df2-43fc-bae4-15f5b7113191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(inputs={\"train\": train_input, \"validation\": test_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840e11d",
   "metadata": {},
   "source": [
    "#### Downloading and Extracting the Artifacts\n",
    "\n",
    "After the training job is in status \"Complete\", we can access the output information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = estimator.model_data\n",
    "print(model_s3_uri)\n",
    "\n",
    "output_s3_uri = \"/\".join(model_s3_uri.split(\"/\")[:-1]) + \"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output_s3_uri ./tmp/train_output/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be838a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./tmp/train_output/output.tar.gz -C ./tmp/train_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381a7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "escrow_model_uri = json.load(open('./tmp/train_output/manifest.json'))['checkpoint_s3_bucket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "escrow_model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0d2fb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ccfa57",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11ce3c",
   "metadata": {},
   "source": [
    "Create minimal recipe for `gen_qa` evaluation. With `gen_qa` evaluation, we bring our own dataset for evaluation, and measure the following metrics:\n",
    "\n",
    "* rouge1\n",
    "* rouge2\n",
    "* rougeL\n",
    "* exact_match\n",
    "* quasi_exact_match\n",
    "* f1_score\n",
    "* f1_score_quasi\n",
    "* bleu\n",
    "\n",
    "Your fine-tuned model checkpoints are accessible through the `manifest.json` in the output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_content = f\"\"\"\n",
    "run:\n",
    "  name: nova-lite-gen_qa-eval-job\n",
    "  model_type: amazon.nova-lite-v1:0:300k\n",
    "  model_name_or_path: {escrow_model_uri}\n",
    "  data_s3_path: {val_dataset_s3_path} # Required, input data s3 location\n",
    "\n",
    "evaluation:\n",
    "  task: gen_qa\n",
    "  strategy: gen_qa\n",
    "  metric: all\n",
    "\n",
    "inference:\n",
    "  max_new_tokens: 4096\n",
    "  top_p: 0.9\n",
    "  temperature: 0.1\n",
    "\"\"\"\n",
    "\n",
    "with open(\"eval-recipe.yaml\", \"w\") as f:\n",
    "  f.write(recipe_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352eaf77",
   "metadata": {},
   "source": [
    "Let's define our PyTorch estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066482cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\" # Override the instance type if you want to get a different container version\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f848c",
   "metadata": {},
   "source": [
    "Let's define the container to execute the Evaluation workload for Amazon Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340c66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nova-lite/prod\"\n",
    "recipe = \"./eval-recipe.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-sft-eval\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,\n",
    "    },\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c1fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "eval_input = TrainingInput(\n",
    "    s3_data=val_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c302fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(inputs={\"train\": eval_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48289760",
   "metadata": {},
   "source": [
    "#### Downloading and Extracting the Artifacts\n",
    "\n",
    "After the training job is in status \"Complete\", we can access the output information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6252911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = estimator.model_data\n",
    "print(model_s3_uri)\n",
    "\n",
    "output_s3_uri = \"/\".join(model_s3_uri.split(\"/\")[:-1]) + \"/output.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f964ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/eval_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output_s3_uri ./tmp/eval_output/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a669244",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./tmp/eval_output/output.tar.gz -C ./tmp/eval_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b6c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"./tmp/eval_output/nova-lite-gen_qa-eval-job/eval_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc400f",
   "metadata": {},
   "source": [
    "### Visualize results\n",
    "\n",
    "After the job is complete, we can visualize our results by using the following utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23488ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def plot_metrics(results):\n",
    "    # Extract metrics and their standard errors\n",
    "    metrics = {}\n",
    "    for key, value in results.items():\n",
    "        if not key.endswith(\"_stderr\"):\n",
    "            metrics[key] = {\"value\": value, \"stderr\": results.get(f\"{key}_stderr\", 0)}\n",
    "\n",
    "    # Sort metrics by value for better visualization\n",
    "    sorted_metrics = dict(\n",
    "        sorted(metrics.items(), key=lambda x: x[1][\"value\"], reverse=True)\n",
    "    )\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    labels = list(sorted_metrics.keys())\n",
    "    values = [sorted_metrics[label][\"value\"] for label in labels]\n",
    "    errors = [sorted_metrics[label][\"stderr\"] for label in labels]\n",
    "\n",
    "    # Normalize BLEU score to be on the same scale as other metrics (0-1)\n",
    "    bleu_index = labels.index(\"bleu\") if \"bleu\" in labels else -1\n",
    "    if bleu_index >= 0:\n",
    "        values[bleu_index] /= 100\n",
    "        errors[bleu_index] /= 100\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create bar chart\n",
    "    x = np.arange(len(labels))\n",
    "    bars = ax.bar(\n",
    "        x,\n",
    "        values,\n",
    "        yerr=errors,\n",
    "        align=\"center\",\n",
    "        alpha=0.7,\n",
    "        capsize=5,\n",
    "        color=\"skyblue\",\n",
    "        ecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Evaluation Metrics\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylim(0, 1.0)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        # Convert BLEU back to its original scale for display\n",
    "        display_value = values[i] * 100 if labels[i] == \"bleu\" else values[i]\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{display_value:.2f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Add a note about BLEU\n",
    "    if bleu_index >= 0:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            -0.15,\n",
    "            \"Note: BLEU score shown as percentage (original: {:.2f})\".format(\n",
    "                values[bleu_index] * 100\n",
    "            ),\n",
    "            transform=ax.transAxes,\n",
    "            ha=\"center\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d759819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c84a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(results_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5457e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_metrics(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", 'evaluation_metrics.png')\n",
    "fig.savefig(output_file, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c3b3e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad90b38",
   "metadata": {},
   "source": [
    "## Model deployment and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e7ce33",
   "metadata": {},
   "source": [
    "After training and evaluating our model, we want to make it available for inference. Amazon Bedrock provides a serverless endpoint for model deployment, allowing us to serve the model without managing infrastructure.\n",
    "\n",
    "The Bedrock Custom Model feature of Amazon Bedrock lets us import our fine-tuned model and access it through the same API as other foundation models. This provides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a17be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client(\"bedrock\", region_name=sess.boto_region_name)\n",
    "\n",
    "model_path = \"<ESCROW_S3_PATH_MODEL_CHECKPOINTS>\"\n",
    "\n",
    "# Define name for imported model\n",
    "imported_model_name = \"nova-lite-sagemaker-sft\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa35ec",
   "metadata": {},
   "source": [
    "### Creating the Bedrock Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789444fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_params = {\n",
    "    \"modelName\": imported_model_name,\n",
    "    \"modelSourceConfig\": {\"s3DataSource\": {\"s3Uri\": model_path}},\n",
    "    \"roleArn\": role,\n",
    "    \"clientRequestToken\": \"NovaRecipeSageMaker\",\n",
    "}\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_custom_model(**request_params)\n",
    "\n",
    "model_arn = response[\"modelArn\"]\n",
    "\n",
    "# Output the model ARN\n",
    "print(f\"Model import job created with ARN: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c836de",
   "metadata": {},
   "source": [
    "### Monitoring the Model status\n",
    "\n",
    "After initiating the model import, we need to monitor its progress. The status goes through several states:\n",
    "\n",
    "* CREATING: Model is being imported\n",
    "* ACTIVE: Import successful\n",
    "* FAILED: Import encountered errors\n",
    "\n",
    "This cell polls the Bedrock API every 60 seconds to check the status of the model import, continuing until it reaches a terminal state (ACTIVE or FAILED). Once the import completes successfully, we'll have the model ARN which we can use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7252c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    response = bedrock.list_custom_models(sortBy='CreationTime',sortOrder='Descending')\n",
    "    model_summaries = response[\"modelSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"modelName\"] == imported_model_name:\n",
    "            status = model[\"modelStatus\"].upper()\n",
    "            model_arn = model[\"modelArn\"]\n",
    "            print(f'{model[\"modelStatus\"].upper()} {model[\"modelArn\"]} ...')\n",
    "            if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "    \n",
    "model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd07844",
   "metadata": {},
   "source": [
    "##### ⚠️ After the model is ACTIVE, deploy a custom model for on-demand inference!\n",
    "\n",
    "Please refer to the official [AWS Documentation](https://docs.aws.amazon.com/nova/latest/userguide/deploy-custom-model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b271f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_params = {\n",
    "    \"clientRequestToken\": \"NovaRecipeSageMakerODI\",\n",
    "    \"modelDeploymentName\": f\"{imported_model_name}-odi\",\n",
    "    \"modelArn\": model_arn,\n",
    "}\n",
    "\n",
    "response = bedrock.create_custom_model_deployment(**request_params)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f799d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "while True:\n",
    "    response = bedrock.list_custom_model_deployments(\n",
    "        sortBy=\"CreationTime\", sortOrder=\"Descending\"\n",
    "    )\n",
    "    model_summaries = response[\"modelDeploymentSummaries\"]\n",
    "    status = \"\"\n",
    "    for model in model_summaries:\n",
    "        if model[\"customModelDeploymentName\"] == f\"{imported_model_name}-odi\":\n",
    "            status = model[\"status\"].upper()\n",
    "            custom_model_arn = model[\"customModelDeploymentArn\"]\n",
    "            print(f'{model[\"status\"].upper()} {model[\"customModelDeploymentArn\"]} ...')\n",
    "            if status in [\"CREATING\"]:\n",
    "                break\n",
    "    if status in [\"ACTIVE\", \"FAILED\"]:\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "\n",
    "custom_model_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981a229",
   "metadata": {},
   "source": [
    "### Testing the Deployed Model\n",
    "\n",
    "Now that our model is deployed to Amazon Bedrock, we can invoke it for inference. We'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cdade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sess.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a719fb",
   "metadata": {},
   "source": [
    "Use the custom model deployment ARN created with the Bedrock Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_arn = (\n",
    "    custom_model_arn\n",
    "    if custom_model_arn is not None\n",
    "    else \"<CUSTOM_MODEL_DEPLOYMENT_ARN>\"\n",
    ")\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that can answer questions and provide information.\n",
    "You can use tools to help you with your tasks.\n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "<tools>\n",
    "{{tools}}\n",
    "</tools>\n",
    "For each function call, return a json object with function name and parameters:\n",
    "\n",
    "{{{{\\\"name\\\": \\\"function name\\\", \\\"parameters\\\": \\\"dictionary of argument name and its value\\\"}}}}\n",
    "\"\"\"\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"calculate_bmi\",\n",
    "            \"description\": \"Calculate BMI given weight in kg and height in meters\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"weight_kg\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Property weight_kg\",\n",
    "                        },\n",
    "                        \"height_m\": {\n",
    "                            \"type\": \"number\",\n",
    "                            \"description\": \"Property height_m\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"weight_kg\", \"height_m\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"description\": 'Fetch weather information\\n\\nArgs:\\nquery: The weather query (e.g., \"weather in New York\")\\nnum_results: Number of results to return (default: 1)\\n\\nReturns:\\nJSON string containing weather information',\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"Property query\",\n",
    "                            },\n",
    "                            \"num_results\": {\n",
    "                                \"type\": \"integer\",\n",
    "                                \"description\": \"Property num_results\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"query\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\n",
    "system_prompt = system_prompt.format(tools=json.dumps({\"tools\": tools}))\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [{\"text\": \"What is the weather in Rome, Italy?\"}]},\n",
    "]\n",
    "\n",
    "response = generate(\n",
    "    model_id=model_arn,\n",
    "    system_prompt=system_prompt,\n",
    "    messages=messages,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "response[\"output\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc096e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354aff9",
   "metadata": {},
   "source": [
    "## LLM as a judge\n",
    "\n",
    "We are now going to evaluate the model with the LLM as a Judge evaluation task. First, let's define the Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afea13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=sess.boto_region_name,\n",
    "    config=Config(\n",
    "        connect_timeout=300,  # 5 minutes\n",
    "        read_timeout=300,  # 5 minutes\n",
    "        retries={\"max_attempts\": 3},\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff17044",
   "metadata": {},
   "source": [
    "To invoke the fine-tuned model for inference, we'll set up the necessary clients and functions to interact with our model through the Bedrock Runtime API.\n",
    "\n",
    "Inference Setup Components:\n",
    "* Bedrock Runtime Client: AWS SDK client for making inference calls\n",
    "* Helper Function: To handle retry logic and properly format requests\n",
    "The generate function we're defining:\n",
    "\n",
    "Applies the proper chat template to user messages\n",
    "* Handles retry logic for robustness\n",
    "* Sets appropriate generation parameters like temperature and top-p\n",
    "\n",
    "This setup allows us to easily test how well our training worked by sending queries to the model and evaluating its responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a2b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate(\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompt=None,\n",
    "    tools=None,\n",
    "    temperature=0.3,\n",
    "    max_tokens=4096,\n",
    "    top_p=0.9,\n",
    "    max_retries=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        model_id (str): ID of the model to use\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        system_prompt (str, optional): System prompt to guide the model\n",
    "        tools (dict, optional): Tool configuration for the model\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    # Prepare base parameters for the API call\n",
    "    kwargs = {\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add optional parameters if provided\n",
    "    if tools:\n",
    "        kwargs[\"toolConfig\"] = tools\n",
    "    if system_prompt:\n",
    "        kwargs[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Retry logic\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return client.converse(modelId=model_id, messages=messages, **kwargs)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(30)\n",
    "            else:\n",
    "                print(\"Max retries reached. Unable to get response.\")\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dad1ab3",
   "metadata": {},
   "source": [
    "#### Generate model inference on the created validation dataset\n",
    "\n",
    "In the following cell, we are going to invoke the model to create the dataset for LLM as a judge.\n",
    "\n",
    "The required dataset structure for LLM as a judge task is:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"String containing the input prompt and instructions.\",\n",
    "    \"response_A\": \"String containing the ground truth output\",\n",
    "    \"response_B\": \"String containing the customized model output.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dea3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids = [\n",
    "    (\n",
    "        custom_model_arn\n",
    "        if custom_model_arn is not None\n",
    "        else \"<CUSTOM_MODEL_DEPLOYMENT_ARN>\"\n",
    "    ),\n",
    "    \"us.amazon.nova-micro-v1:0\",\n",
    "]\n",
    "\n",
    "# Change model ID to run LLM as a judge evaluation on the fine-tuned model or base model\n",
    "model_id = model_ids[0]\n",
    "\n",
    "llm_val_dataset = []\n",
    "\n",
    "index = 1\n",
    "for el in val_dataset:\n",
    "    print(\"Processing row \", index)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": el[\"query\"]}]},\n",
    "    ]\n",
    "\n",
    "    response = generate(\n",
    "        model_id=model_id,\n",
    "        system_prompt=el[\"system\"],\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    question = (\n",
    "        el[\"system\"] + \"\\n\\n\" + el[\"query\"]\n",
    "        if el[\"system\"] != \"\"\n",
    "        else el[\"query\"]\n",
    "    )\n",
    "\n",
    "    llm_val_dataset.append(\n",
    "        [\n",
    "            question,\n",
    "            el[\"response\"],\n",
    "            response[\"output\"][\"message\"][\"content\"][0][\"text\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    index += 1\n",
    "\n",
    "llm_judge_df = pd.DataFrame(\n",
    "    llm_val_dataset, columns=[\"prompt\", \"response_A\", \"response_B\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf1475",
   "metadata": {},
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37293292",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/nova-dpo\"\n",
    "else:\n",
    "    input_path = f\"datasets/nova-dpo\"\n",
    "\n",
    "llm_judge_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/llm-judge/llm_judge.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save datasets to s3\n",
    "os.makedirs(\"./data/llm-judge\", exist_ok=True)\n",
    "\n",
    "llm_judge_df.to_json(\"./data/llm-judge/llm_judge.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "s3_client.upload_file(\n",
    "    \"./data/llm-judge/llm_judge.jsonl\",\n",
    "    bucket_name,\n",
    "    f\"{input_path}/llm-judge/llm_judge.jsonl\",\n",
    ")\n",
    "\n",
    "#shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(llm_judge_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a2a3c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf86e88d",
   "metadata": {},
   "source": [
    "### Run LLM as a Judge evaluation job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68edad81",
   "metadata": {},
   "source": [
    "Create minimal recipe for `LLM-as-Judge` evaluation. With `llm_judge` evaluation, we bring our own dataset for evaluation, and measure the following metrics:\n",
    "\n",
    "* a_scores\n",
    "* b_scores\n",
    "* ties\n",
    "* inference_error\n",
    "* score\n",
    "* winrate\n",
    "* lower_rate\n",
    "* upper_rate\n",
    "\n",
    "Your fine-tuned model checkpoints are accessible through the `manifest.json` in the output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c5ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_content = f\"\"\"\n",
    "run:\n",
    "  name: nova-micro-llm-judge-eval-job\n",
    "  model_type: amazon.nova-micro-v1:0:128k\n",
    "  model_name_or_path: \"nova-micro/prod\"\n",
    "  replicas: 1 # unmodifiable\n",
    "  data_s3_path: {llm_judge_dataset_s3_path} # Required, input data s3 location\n",
    "\n",
    "evaluation:\n",
    "  task: llm_judge # not modifiable\n",
    "  strategy: judge # not modifiable\n",
    "  metric: all # not modifiable\n",
    "\n",
    "inference:\n",
    "  max_new_tokens: 4096 # modifiable\n",
    "  top_p: 0.9 # modifiable\n",
    "  temperature: 0.1 # modifiable\n",
    "\"\"\"\n",
    "\n",
    "with open(\"llm-judge-recipe.yaml\", \"w\") as f:\n",
    "  f.write(recipe_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e5282",
   "metadata": {},
   "source": [
    "Let's define our PyTorch estimator, by pointing to the created evaluation recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ba3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g5.12xlarge\" # Override the instance type if you want to get a different container version\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91999d7b",
   "metadata": {},
   "source": [
    "Let's define the container to execute the Evaluation workload for Amazon Nova models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae95ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = f\"708977205387.dkr.ecr.{sess.boto_region_name}.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-latest\"\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed2fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"nova-lite/prod\"\n",
    "recipe = \"./llm-judge-recipe.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[0].replace('.', '-')}-dpo-peft-llm-judge\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "recipe_overrides = {\n",
    "    \"run\": {\n",
    "        \"replicas\": instance_count,\n",
    "    },\n",
    "}\n",
    "\n",
    "estimator = PyTorch(\n",
    "    output_path=output_path,\n",
    "    base_job_name=job_name,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    training_recipe=recipe,\n",
    "    recipe_overrides=recipe_overrides,\n",
    "    max_run=432000,\n",
    "    sagemaker_session=sess,\n",
    "    image_uri=image_uri,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "eval_input = TrainingInput(\n",
    "    s3_data=llm_judge_dataset_s3_path,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "\n",
    "eval_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "estimator.fit(inputs={\"train\": eval_input}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8e7ad",
   "metadata": {},
   "source": [
    "#### Downloading and Extracting the Artifacts\n",
    "\n",
    "After the training job is in status \"Complete\", we can access the output information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db52886",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./tmp/llm_judge_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae011414",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $output_s3_uri ./tmp/llm_judge_output/output.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa6759",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./tmp/llm_judge_output/output.tar.gz -C ./tmp/llm_judge_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b7951",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"./tmp/llm_judge_output/nova-lite-llm-judge-eval-job/eval_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a5ef0",
   "metadata": {},
   "source": [
    "#### Visualize results\n",
    "\n",
    "After the job is complete, we can visualize our results by using the following utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b415d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_llm_judge_results(results):\n",
    "    \"\"\"\n",
    "    Plot LLM judge evaluation results with bar chart and pie chart only.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Dictionary containing evaluation results\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.pyplot: The pyplot object with the plots\n",
    "    \"\"\"\n",
    "    # Set style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 1. Score Distribution Bar Chart\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    scores = {\n",
    "        \"A Scores\": results[\"a_scores\"],\n",
    "        \"B Scores\": results[\"b_scores\"],\n",
    "        \"Ties\": results[\"ties\"],\n",
    "        \"Inference Errors\": results[\"inference_error\"],\n",
    "    }\n",
    "\n",
    "    bars = ax1.bar(\n",
    "        scores.keys(),\n",
    "        scores.values(),\n",
    "        color=[\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\", \"#FFA07A\"],\n",
    "    )\n",
    "    ax1.set_title(\"Score Distribution\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.set_ylabel(\"Count\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, scores.values()):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + height * 0.01,\n",
    "            f\"{int(value)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # 2. Preference Pie Chart (excluding inference errors)\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    total_valid = results[\"a_scores\"] + results[\"b_scores\"] + results[\"ties\"]\n",
    "\n",
    "    if total_valid > 0:\n",
    "        pie_data = [results[\"a_scores\"], results[\"b_scores\"], results[\"ties\"]]\n",
    "        pie_labels = [\"A Preferred\", \"B Preferred\", \"Ties\"]\n",
    "        colors = [\"#FF6B6B\", \"#4ECDC4\", \"#45B7D1\"]\n",
    "\n",
    "        wedges, texts, autotexts = ax2.pie(\n",
    "            pie_data, labels=pie_labels, colors=colors, autopct=\"%1.1f%%\", startangle=90\n",
    "        )\n",
    "\n",
    "        # Make percentage text bold\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_fontweight(\"bold\")\n",
    "            autotext.set_color(\"white\")\n",
    "\n",
    "    ax2.set_title(\n",
    "        \"Preference Distribution\\n(Valid Judgments Only)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af69ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def find_json_files(path):\n",
    "    return glob.glob(os.path.join(path, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results_path = find_json_files(results_path)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(evaluation_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fig = plot_llm_judge_results(data[\"results\"][\"all\"])\n",
    "\n",
    "output_file = os.path.join(\"./\", \"evaluation_metrics_llm_judge.png\")\n",
    "fig.savefig(output_file, bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova-sj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
