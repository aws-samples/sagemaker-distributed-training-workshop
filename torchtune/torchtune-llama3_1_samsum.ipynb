{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0af5654-1925-46a1-be8d-c4b57f59cf26",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3.1 models using torchtune on Amazon SageMaker\n",
    "\n",
    "In this notebook, we are using Metaâ€™s torchtune library to fine-tune Llama 3.1 8B model with LoRA fine-tuning strategies on Amazon SageMaker training. \n",
    "\n",
    "**torchtune** is a Native-PyTorch library that aims to democratize and streamline the fine-tuning process for LLMs, making it easier for researchers, developers, and organizations to adapt these powerful LLMs to their specific needs and constraints. \n",
    "\n",
    "In this use case, we are walking through an end-to-end example on how you can fine-tune a Llama 3.1 8B model with LoRA, run generation in memory, and optionally quantize and evaluate the model  using torchtune and SageMaker training.  \n",
    "\n",
    "Recipes, prompt templates, configs and datasets are completely configurable and allows you to align torchtune to your requirements. To demonstrate this, we will use a custom prompt template in this use case with the open source dataset Samsung/samsum from the Hugging Face hub.\n",
    "\n",
    "We are fine-tune using torchtune multi-device LoRA recipe (lora_finetune_distributed) and use the SageMaker customized version of Llama 3.1 8B  default config (llama3_1/8B_lora)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99414016-e912-4201-bf86-2c92b42dcdbb",
   "metadata": {},
   "source": [
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install torchtune and SageMaker Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b028fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall \"autogluon-multimodal\" \"aiobotocore\" \"amazon-sagemaker-sql-magic\" \"autogluon-core\" \"autogluon-features\" \"autogluon-tabular\" \"autogluon-timeseries\" \"langchain-aws\" \"sparkmagic\" \"virtualenv\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17185d9-8554-4db4-8a48-37851ba243c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker\" \"boto3\" \"datasets\" \"py7zr\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8eb12-c7f6-4bfd-85af-da0cd654a258",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d3628-51cc-4b17-adf0-537b84949621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, time, json\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from typing import Dict, Any\n",
    "from pprint import pprint\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"sagemaker_execution_role\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sagemaker_session is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sagemaker_session_bucket}\")\n",
    "print(f\"sagemaker session region: {sagemaker_session.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e49d5-94c5-4f33-91ca-09b57f9ee5ae",
   "metadata": {},
   "source": [
    "## 1.1 Pre-Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602c1df-bbea-45f3-b0ba-2042417c72d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Samsung/samsum\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728a5a8-7784-4a83-b104-0bfdfa60e741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_sample=dataset['train'].select(range(100))\n",
    "\n",
    "dataset_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6198e-d81a-4fda-ae39-48dc8c623099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_sample.to_json('./dataset/samsum_train.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed1de8-0553-46e3-926e-72763d13c332",
   "metadata": {},
   "source": [
    "## 1.2 Define Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a92e6",
   "metadata": {},
   "source": [
    "In the following cells, we will retrieve the necessary information to be used when configuring the PyTorch Estimator later in step 1.5. As part of the set-up for this workshop, we have created an S3 bucket, an EFS shared file system for you and a VPC including subnets to use for the training job. As these need to be specified later in the Estimator, we need to go ahead and retrieve the details. We will retrieve these information by querying the CloudFormation stack that was deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9986c5ff-a5e6-40d3-8adb-7d1e2179eae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. CF Stack Name\n",
    "stack_name='cf'\n",
    "\n",
    "#2. Region name\n",
    "region= sagemaker_session.boto_region_name\n",
    "\n",
    "#3. Model that we will fine-tune\n",
    "model_id=\"meta-llama/Meta-Llama-3.1-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac1eaf-3f96-46c7-a75e-b55efeafeab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get EFS-id, private-subnet-id and EFS-id for next step of fine-tuning\n",
    "\n",
    "def get_stack_outputs(stack_name, region='us-west-2'):\n",
    "    \"\"\"\n",
    "    Retrieves all outputs from a CloudFormation stack.\n",
    "    \n",
    "    :param stack_name: Name of the CloudFormation stack\n",
    "    :param region: AWS region where the stack is deployed (default is 'us-east-1')\n",
    "    :return: Dictionary of stack outputs\n",
    "    \"\"\"\n",
    "    cfn_client = boto3.client('cloudformation', region_name=region)\n",
    "    \n",
    "    try:\n",
    "        response = cfn_client.describe_stacks(StackName=stack_name)\n",
    "        stack_outputs = response['Stacks'][0]['Outputs']\n",
    "        \n",
    "       # print(stack_outputs)\n",
    "        # Convert the list of outputs to a dictionary for easier access\n",
    "        outputs_dict = {output['OutputKey']: output['OutputValue'] for output in stack_outputs}\n",
    "       \n",
    "\n",
    "        return outputs_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving stack outputs: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "outputs = get_stack_outputs(stack_name, region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915696a-1555-40ae-9945-bb7c54db53a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30ab5c",
   "metadata": {},
   "source": [
    "## 1.3 Define S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517efca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. S3 url with model weights\n",
    "s3_model_artifacts=outputs[\"S3ModelUri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e151cc3-2d2b-4808-b55f-cf5828e6ff0f",
   "metadata": {},
   "source": [
    "## 1.4 Define EFS and networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50f0f4-1a7c-4771-b8b5-ddf40003974e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define one-time network configuration for VPC to use EFS\n",
    "# This example has been optimized and tested on EFS. If you want to use S3, please change the config files to match S3 directory path\n",
    "\n",
    "\n",
    "use_efs=True\n",
    "\n",
    "# VPC config\n",
    "network_config={\n",
    "\n",
    "   \"subnets\": [outputs['SubnetID1'], \n",
    "               outputs['SubnetID2'], \n",
    "               outputs['SubnetID3'], \n",
    "               outputs['SubnetID4'], \n",
    "               outputs['SubnetID5'], \n",
    "               outputs['SubnetID6']],\n",
    "   \"security_group_ids\": [outputs['SecurityGroup']] # e.g. [\"sg-xxxx\"]\n",
    "}\n",
    "\n",
    "# EFS file system id \n",
    "efs_file_system_id=outputs['EFSFileSystemId'] # e.g. 'fs-xxxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3951aa5-d0cb-4d4b-b5b4-7384c6efcdbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "network_config, efs_file_system_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c537ffb-0e27-4548-9eb4-6c9aaf30c962",
   "metadata": {},
   "source": [
    "## 1.5 Define PyTorch Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d188b8",
   "metadata": {},
   "source": [
    "The Estimator handles end-to-end SageMaker training. It is the core element of a SageMaker Training Job. The cells will configure all the settings for the SageMaker Training job, including which PyTorch version to use, which training script to execute and more. For more information on the SageMaker Estimator and a detailed overview about all arguments, please refer to the [Documentation here](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522fdeff-eb4d-4be3-a93b-d5999497d079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "use_downloaded_model = \"true\"\n",
    "\n",
    "def create_pytorch_estimator(**kwargs: Any) -> PyTorch:\n",
    "    \"\"\"\n",
    "    Create a PyTorch estimator for SageMaker training with dynamic configuration.\n",
    "\n",
    "    Args:\n",
    "    **kwargs: Arbitrary keyword arguments for PyTorch estimator configuration.\n",
    "\n",
    "    Returns:\n",
    "    PyTorch: Configured PyTorch estimator.\n",
    "\n",
    "    Raises:\n",
    "    KeyError: If required parameters are missing in kwargs.\n",
    "    \"\"\"        \n",
    "    \n",
    "    job_name = f'torchtune-{kwargs[\"hyperparameters\"][\"tune_action\"]}'\n",
    "    \n",
    "    # Upload configs to S3 folder\n",
    "    inputs = sagemaker_session.upload_data(path=\"config\", bucket=sagemaker_session_bucket, key_prefix=\"config\")\n",
    "    templates = sagemaker_session.upload_data(path=\"custom_template\", bucket=sagemaker_session_bucket, key_prefix=\"templates\")\n",
    "    dataset = sagemaker_session.upload_data(path=\"dataset\", bucket=sagemaker_session_bucket, key_prefix=\"dataset\")\n",
    "\n",
    "    print(\"torchtune configs uploaded to:{} \\n\".format(inputs))\n",
    "    print(\"and to:{} \\n\".format(templates))\n",
    "\n",
    "    env_var = {\n",
    "        \"SAGEMAKER_REQUIREMENTS\": \"requirements.txt\",\n",
    "    }\n",
    "\n",
    "    # Default configuration\n",
    "    estimator_config = {\n",
    "        \"entry_point\": \"launcher.py\",\n",
    "        \"source_dir\": \"./scripts\",\n",
    "        \"base_job_name\": job_name,\n",
    "        \"max_run\": 86400,\n",
    "        \"framework_version\": \"2.4.0\",\n",
    "        \"py_version\": \"py311\",\n",
    "        \"disable_output_compression\": True,\n",
    "        \"keep_alive_period_in_seconds\": 1800,\n",
    "        \"env\": env_var,\n",
    "        \"role\": role,\n",
    "        \"sagemaker_session\": sagemaker_session,\n",
    "        \"disable_profiler\":True,\n",
    "        \"debugger_hook_config\":False\n",
    "    }\n",
    "\n",
    "    # Update with provided kwargs\n",
    "    estimator_config.update(kwargs)\n",
    "\n",
    "    # Ensure required parameters are present\n",
    "    required_params = ['instance_type', 'instance_count', 'hyperparameters']\n",
    "    for param in required_params:\n",
    "        if param not in estimator_config:\n",
    "            raise KeyError(f\"Missing required parameter: {param}\")\n",
    "\n",
    "    # Configure EFS if specified\n",
    "    if use_efs:\n",
    "        required_keys = {'subnets', 'security_group_ids'}\n",
    "        missing_keys = set(required_keys) - set(network_config.keys())\n",
    "        \n",
    "        if missing_keys:\n",
    "            raise ValueError(f\"Missing required keys: {', '.join(missing_keys)}\")\n",
    "    \n",
    "        for key, value in network_config.items():\n",
    "            if value is None or len(value) == 0:\n",
    "                raise ValueError(f\"Missing required value for {key}: {value}\")\n",
    "                \n",
    "        estimator_config.update(network_config)\n",
    "        \n",
    "    # Remove 'use_efs' from config as it's not a PyTorch estimator parameter\n",
    "    estimator_config[\"hyperparameters\"].pop('use_efs', None)\n",
    "    \n",
    "    global use_downloaded_model\n",
    "    use_downloaded_model = estimator_config[\"hyperparameters\"][\"use_downloaded_model\"]\n",
    "    use_downloaded_model=bool(use_downloaded_model) and use_downloaded_model.lower() not in ('false', '0', 'no', 'n', 'off')\n",
    "        \n",
    "    print(\"SageMaker PyTorch Estimator: \\n\")\n",
    "    pprint(estimator_config)\n",
    "\n",
    "    return PyTorch(**estimator_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ead5fb",
   "metadata": {},
   "source": [
    "Now we define a helper function which will execute the SageMaker Job by calling `estimator.fit()`. This will eventually kick off the SageMaker Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c509b-eb52-4d2a-a2be-23fa35a2d984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def execute_task(estimator):\n",
    "    \"\"\"\n",
    "    Execute the task using the provided estimator and input data channels.\n",
    "\n",
    "    Args:\n",
    "    estimator (sagemaker.estimator.Estimator): The SageMaker estimator to use for training.\n",
    "    s3_config_bucket (str): The S3 bucket path for the configuration data.\n",
    "    \"\"\"\n",
    "        \n",
    "    if use_efs:\n",
    "        if efs_file_system_id is None or len(efs_file_system_id) == 0:\n",
    "            raise ValueError(f\"Missing required value for efs_file_system_id: {efs_file_system_id}\")\n",
    "        \n",
    "        # Define the EFS input\n",
    "        efs_input = FileSystemInput(\n",
    "            file_system_id=efs_file_system_id,\n",
    "            file_system_type='EFS',\n",
    "            directory_path='/',\n",
    "            file_system_access_mode='rw'\n",
    "        )\n",
    "    else:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.put_object(Bucket=sagemaker_session_bucket, Key=\"artifacts\")\n",
    "    \n",
    "    s3_config_bucket = f\"s3://{sagemaker_session_bucket}/config\"\n",
    "    s3_custom_template = f\"s3://{sagemaker_session_bucket}/templates\"\n",
    "    s3_model_store = f\"s3://{sagemaker_session_bucket}/artifacts\"\n",
    "    s3_dataset = f\"s3://{sagemaker_session_bucket}/dataset\"\n",
    "\n",
    "    # Define the data channels\n",
    "    data_channels = {\n",
    "        \"config\": s3_config_bucket,\n",
    "        \"model\": efs_input if use_efs else s3_model_store,\n",
    "        \"templates\":s3_custom_template,\n",
    "        \"dataset\":s3_dataset,\n",
    "        \"model_artifacts\": s3_model_artifacts\n",
    "    }\n",
    "    \n",
    "    if use_downloaded_model:\n",
    "        data_channels.pop('model_artifacts', None)\n",
    "\n",
    "    print(f'data_channels:{data_channels}')\n",
    "    \n",
    "    # Fit the estimator with the input data channels\n",
    "    estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6bcec-c599-4dba-b7e7-1640b59575a9",
   "metadata": {},
   "source": [
    "## 1.6 Define SageMaker Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8c12e-c882-46fb-9bc0-0b621f75c5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set common parameters\n",
    "hyperparam_common_values={}\n",
    "\n",
    "hyperparam_common_values[\"model_id\"]=model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb78b9-3b33-45b6-b1b4-1a538dabca6a",
   "metadata": {},
   "source": [
    "Define SageMaker tasks for every specific model customization lifecycle step. Each task defines the configuration of the compute cluster that SageMaker will spin up to run the specific torchtune recipe.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3241c0-5898-4013-a7a1-a27de70e250b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=r'{\"dialogue\":\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure \\r\\nAmanda: I will bring you tomorrow :-)\"}'\n",
    "\n",
    "sagemaker_tasks={}\n",
    "\n",
    "# Define SageMaker task that will create a specifc SageMaker PyTorch estimator for a torchtune recipe\n",
    "# Make sure keys are defined in the same format \n",
    "sagemaker_tasks={\n",
    "    \"fine-tune\":{\n",
    "        \"hyperparameters\":{\n",
    "            \"tune_config_name\":\"config_l3.1_8b_qlora.yaml\",\n",
    "            \"tune_action\":\"fine-tune\",\n",
    "            \"use_downloaded_model\":\"false\",\n",
    "            \"tune_recipe\":\"lora_finetune_distributed\" # check torchtune documentation or run \"tune ls\" to find all recipes available\n",
    "            },\n",
    "        \"instance_count\":1,\n",
    "        \"instance_type\":\"ml.g5.2xlarge\",  \n",
    "    },\n",
    "    \"generate_inference_on_trained\":{\n",
    "        \"hyperparameters\":{\n",
    "            \"tune_config_name\":\"config_l3.1_8b_gen_trained.yaml\",\n",
    "            \"tune_action\":\"generate-trained\",\n",
    "            \"use_downloaded_model\":\"true\",\n",
    "            #\"prompt\":json.dumps(prompt)\n",
    "            },\n",
    "        \"instance_count\":1,\n",
    "        \"instance_type\":\"ml.g5.2xlarge\",\n",
    "    },\n",
    "    \"generate_inference_on_original\":{\n",
    "        \"hyperparameters\":{\n",
    "            \"tune_config_name\":\"config_l3.1_8b_gen_orig.yaml\",\n",
    "            \"tune_action\":\"generate-original\",\n",
    "            \"use_downloaded_model\":\"true\",\n",
    "            #\"prompt\":json.dumps(prompt)\n",
    "            },\n",
    "        \"instance_count\":1,\n",
    "        \"instance_type\":\"ml.g5.2xlarge\",\n",
    "    },\n",
    "    \"evaluate_trained_model\":{\n",
    "        \"hyperparameters\":{\n",
    "            \"tune_config_name\":\"config_l3.1_8b_eval_trained.yaml\",\n",
    "            \"tune_action\":\"run-eval\",\n",
    "            \"use_downloaded_model\":\"true\",\n",
    "            \"prompt\":json.dumps(prompt)\n",
    "            },\n",
    "        \"instance_count\":1,\n",
    "        \"instance_type\":\"ml.g5.2xlarge\",\n",
    "    }\n",
    "}\n",
    "\n",
    "for k,v in sagemaker_tasks.items():\n",
    "    sagemaker_tasks[k][\"hyperparameters\"].update(hyperparam_common_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94a782-ccc7-4bc7-99f4-f9118c19bcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e0be6-82f9-46d9-9777-7008a3c2d140",
   "metadata": {},
   "source": [
    "## 2. Fine Tune Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1b455",
   "metadata": {},
   "source": [
    "Now that we have set everything up and defined the tasks, it is time to execute the fine-tuning job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8c7345",
   "metadata": {},
   "source": [
    "Before the job is executed by SageMaker, you can take a look at the torchtune recipe which we are using for the Fine-Tuning task. There you will see all configurations which are used in this task, including the `model`, `tokenizer`, `checkpointer`, `profiler`, `optimizer` and additional specifications for the training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display torchtune config .yaml file\n",
    "!pygmentize ./config/config_l3.1_8b_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50333917-2f58-4cb2-9051-79e7fd87ed67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"  *** TASK for the Job. Select one of the below tasks: ***\n",
    "  {fine-tune, generate_inference_on_trained,generate_inference_on_original,quantize_trained_model, \n",
    "   generate_inference_on_trained_quant,evaluate_trained_model} \"\"\"\n",
    "    \n",
    "Task=\"fine-tune\"\n",
    "\n",
    "# Optionally print or override the task dictionary\n",
    "#pprint(sagemaker_tasks[Task])\n",
    "\n",
    "estimator=create_pytorch_estimator(**sagemaker_tasks[Task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d86dada-7684-461e-9354-57c43736c940",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_task(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d307e00",
   "metadata": {},
   "source": [
    "While the job is executed by SageMaker, you will see output logs being printed. Please continue to the next step once the training has finished successfully!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fd603-eb2c-451e-bed5-611636f310aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Generate Trained Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0b6185",
   "metadata": {},
   "source": [
    "After the model is now fine-tuned, let's try to generate some inference on that trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29c36d",
   "metadata": {},
   "source": [
    "Again, before the job is executed by SageMaker, you can take a look at the torchtune recipe which we are using for the Infernece task. There you will see all configurations which are used in this task, including the `model`, `tokenizer`, `checkpointer`, `profiler`, `optimizer` and additional specifications for the inference run. \n",
    "\n",
    "If you wish to do so, please un-comment the code in the next cell. Otherwise, please continue to the next step to run the Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f1306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display torchtune config .yaml file\n",
    "\n",
    "#!pygmentize ./config/config_l3.1_8b_gen_trained.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc62485-2663-4342-b72e-d9bafa499330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"  *** TASK for the Job. Select one of the above below tasks: ***\n",
    "  {fine-tune, generate_inference_on_trained,generate_inference_on_original,quantize_trained_model, \n",
    "   generate_inference_on_trained_quant,evaluate_trained_model} \"\"\"\n",
    "\n",
    "Task=\"generate_inference_on_trained\" \n",
    "\n",
    "# You can overwrite any parameters in the SageMaker task as you see fit for your experimentation\n",
    "prompt=r'{\"dialogue\":\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure \\r\\nAmanda: I will bring you tomorrow :-)\"}'\n",
    "\n",
    "sagemaker_tasks[Task]['hyperparameters']['prompt']=json.dumps(prompt)\n",
    "\n",
    "estimator=create_pytorch_estimator(**sagemaker_tasks[Task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03642d0f-d3ee-4d4e-ac75-a2a815654fca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_task(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d863e",
   "metadata": {},
   "source": [
    "**Congratulations, you have successfully completed this workshop and learnt how to fine-tune Llama3 deep learning models on Amazon SageMaker with limited data and compute resources.**\n",
    "\n",
    "Optionally, you can also run inference and evaluation on the original, non-fine-tuned Llama 3.1 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e477130-2407-4a71-a39b-9e9ee029308f",
   "metadata": {},
   "source": [
    "## 3.2 Generate Original Model Inference **[OPTIONAL]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8fcbb",
   "metadata": {},
   "source": [
    "Again, before the job is executed by SageMaker, you can take a look at the torchtune recipe which we are using for the Inference task. There you will see all configurations which are used in this task, including the `model`, `tokenizer`, `checkpointer`, `profiler`, `optimizer` and additional specifications for the evaluation run. \n",
    "\n",
    "If you wish to do so, please un-comment the code in the next cell. Otherwise, please continue to the next step to run the Inference job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display torchtune config .yaml file\n",
    "\n",
    "#!pygmentize ./config/config_l3.1_8b_gen_original.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6185b7-1427-400a-a717-8d1199796b78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"  *** TASK for the Job. Select one of the above below tasks: ***\n",
    "  {fine-tune, generate_inference_on_trained,generate_inference_on_original,quantize_trained_model, \n",
    "   generate_inference_on_trained_quant,evaluate_trained_model} \"\"\"\n",
    "\n",
    "Task=\"generate_inference_on_original\" \n",
    "\n",
    "# You can overwrite any parameters in the SageMaker task as you see fit for your experimentation\n",
    "prompt=r'{\"dialogue\":\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure \\r\\nAmanda: I will bring you tomorrow :-)\"}'\n",
    "\n",
    "sagemaker_tasks[Task]['hyperparameters']['prompt']=json.dumps(prompt)\n",
    "\n",
    "#pprint(sagemaker_tasks[Task])\n",
    "\n",
    "estimator=create_pytorch_estimator(**sagemaker_tasks[Task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d80b3-53c2-4de7-a452-4c7013cdd7ff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_task(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8947846-606d-445f-b0b4-8b61a51162f6",
   "metadata": {},
   "source": [
    "## 4.1 Evaluate Trained Model **[OPTIONAL]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b516794",
   "metadata": {},
   "source": [
    "After getting an intuitive overview about the performance of the fine-tuned model in the previous step, let's now evaluate them in all objectivity. Create an Evaluation Job for the fine-tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce34624",
   "metadata": {},
   "source": [
    "Again, before the job is executed by SageMaker, you can take a look at the torchtune recipe which we are using for the Evaluation task. There you will see all configurations which are used in this task, including the `model`, `tokenizer`, `checkpointer`, `profiler`, `optimizer` and additional specifications for the evaluation run. \n",
    "\n",
    "If you wish to do so, please un-comment the code in the next cell. Otherwise, please continue to the next step to run the Evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c86904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display torchtune config .yaml file\n",
    "\n",
    "#!pygmentize ./config/config_l3.1_8b_eval_trained.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e900af-4569-4aa5-b69e-df01eec3b74f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"  *** TASK for the Job. Select one of the above below tasks: ***\n",
    "  {fine-tune, generate_inference_on_trained,generate_inference_on_original,quantize_trained_model, \n",
    "   generate_inference_on_trained_quant,evaluate_trained_model} \"\"\"\n",
    "\n",
    "Task=\"evaluate_trained_model\" \n",
    "\n",
    "sagemaker_tasks[Task]['hyperparameters']['tune_config_name']='config_l3.1_8b_eval_trained.yaml'\n",
    "\n",
    "estimator=create_pytorch_estimator(**sagemaker_tasks[Task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d10eff-be2b-4627-b4c0-33f8132a83a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_task(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1516ad-aa22-4217-84c7-76d5ea3da795",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate Original Model **[OPTIONAL]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3ce82",
   "metadata": {},
   "source": [
    "Again, before the job is executed by SageMaker, you can take a look at the torchtune recipe which we are using for the Evaluation task. There you will see all configurations which are used in this task, including the `model`, `tokenizer`, `checkpointer`, `profiler`, `optimizer` and additional specifications for the evaluation run. \n",
    "\n",
    "If you wish to do so, please un-comment the code in the next cell. Otherwise, please continue to the next step to run the Evaluation job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30be9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display torchtune config .yaml file\n",
    "\n",
    "#!pygmentize ./config/config_l3.1_8b_eval_original.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005e839-a2ce-499c-966c-9dcd658a96a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"  *** TASK for the Job. Select one of the above below tasks: ***\n",
    "  {fine-tune, generate_inference_on_trained,generate_inference_on_original,quantize_trained_model, \n",
    "   generate_inference_on_trained_quant,evaluate_trained_model} \"\"\"\n",
    "\n",
    "Task=\"evaluate_trained_model\" \n",
    "\n",
    "sagemaker_tasks[Task]['hyperparameters']['tune_config_name']='config_l3.1_8b_eval_original.yaml'\n",
    "\n",
    "estimator=create_pytorch_estimator(**sagemaker_tasks[Task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b70ff9-ced0-4541-bcad-78b23ef96ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_task(estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
