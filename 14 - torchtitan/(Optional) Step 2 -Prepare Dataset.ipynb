{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557d9ff5-b8cb-4180-811e-aa2e23b788ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## (Optional) Step 2: Prepare your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01fec4-a6ec-4675-8c1f-d5e5d319fb2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "By default the torchtitan library uses the allenai/c4 dataset in its training configuration. This is streamed directly during training. \n",
    "\n",
    "However, you may want to pre-train the LLama-3 models on your own dataset residing in s3. In this notebook, we will download the allenai/c4 dataset to s3 and in the training notebook , we will show you how you can configure the torchtitan library to use the dataset residing in s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e52fa-ab8c-49d1-97b0-b6c66e4dcaff",
   "metadata": {},
   "source": [
    "We first create a processing script to download the dataset from HuggingFace in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e40472-ba1e-4c09-837d-f945dbd53679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing download_c4_dataset.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile download_c4_dataset.py\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"datasets\", \"tqdm\"])\n",
    "\n",
    "\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def process_and_upload_chunk(args):\n",
    "    output_path, chunk_id, chunk = args\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing chunk {chunk_id}\")\n",
    "        # Convert chunk to Arrow table\n",
    "        arrow_table = pa.Table.from_pylist(chunk)\n",
    "        \n",
    "        # Write the table to an in-memory buffer as Parquet\n",
    "        buf = io.BytesIO()\n",
    "        pq.write_table(arrow_table, buf)\n",
    "        \n",
    "        # Get the bytes from the buffer\n",
    "        parquet_bytes = buf.getvalue()\n",
    "        \n",
    "        # Save to local file\n",
    "        file_path = os.path.join(output_path, f\"chunk_{chunk_id:06d}.parquet\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(parquet_bytes)\n",
    "        \n",
    "        return chunk_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk {chunk_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Process C4 dataset')\n",
    "    parser.add_argument('--input-data', type=str, help='S3 path to input data (optional)')\n",
    "    parser.add_argument('--output-dir', type=str, default='/opt/ml/processing/output', help='Output directory')\n",
    "    parser.add_argument('--chunk-size', type=int, default=10000, help='Number of examples per chunk')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_path = args.output_dir\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    print(\"Loading C4 dataset...\")\n",
    "    dataset_args = {\"streaming\": True}\n",
    "    if args.input_data:\n",
    "        dataset_args[\"data_files\"] = args.input_data\n",
    "    dataset = load_dataset(\"c4\", \"en\", split=\"train\", **dataset_args)\n",
    "\n",
    "    print(f\"Saving dataset to {output_path}\")\n",
    "\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print(f\"Number of CPUs available: {num_cpus}\")\n",
    "\n",
    "    chunk_size = args.chunk_size\n",
    "    chunk = []\n",
    "    chunk_id = 0\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_cpus) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        with tqdm(total=None) as pbar:\n",
    "            for example in dataset:\n",
    "                chunk.append(example)\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    future_args = (output_path, chunk_id, chunk)\n",
    "                    futures.append(executor.submit(process_and_upload_chunk, future_args))\n",
    "                    chunk = []\n",
    "                    chunk_id += 1\n",
    "                    pbar.update(chunk_size)\n",
    "\n",
    "            if chunk:\n",
    "                future_args = (output_path, chunk_id, chunk)\n",
    "                futures.append(executor.submit(process_and_upload_chunk, future_args))\n",
    "                pbar.update(len(chunk))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                print(f\"Completed processing chunk {result}\")\n",
    "\n",
    "    print(\"Dataset processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df659c7e-4f88-4575-9cb1-ee49ea1a3311",
   "metadata": {},
   "source": [
    "We then launch the above script using a SageMaker processsing job to download the dataset to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da564db0-122a-489f-a8c4-dac01c45d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "# Setup\n",
    "role = get_execution_role()\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: {account}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "print(f\"AWS region: {region}\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "# get default bucket\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(\"Default bucket for this session: \", default_bucket)\n",
    "\n",
    "# Create a custom Processor\n",
    "script_processor = ScriptProcessor(\n",
    "    role=role,\n",
    "    image_uri=sagemaker.image_uris.retrieve(\n",
    "        framework=\"sklearn\",\n",
    "        region=region,\n",
    "        version=\"0.23-1\",\n",
    "    ),\n",
    "    instance_count=20,\n",
    "    instance_type=\"ml.c5.9xlarge\",\n",
    "    base_job_name='c4-dataset-processing',\n",
    "    command=[\"python3\"],\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "# Set up the processing job\n",
    "script_processor.run(\n",
    "    code=\"download_c4_dataset.py\",\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"c4_dataset\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"s3://{default_bucket}/c4-dataset\",\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        \"--output-dir\", \"/opt/ml/processing/output\",\n",
    "        \"--chunk-size\", \"100000\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Processing job '{script_processor.latest_job.job_name}' started.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d5fb37-1f2c-4dc4-a6ef-e16c23fae5fd",
   "metadata": {},
   "source": [
    "We then launch the above script using a SageMaker processsing job to download the dataset to s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ff1060-41d6-4c7f-bb53-be68f3122df2",
   "metadata": {},
   "source": [
    "Please note down the s3 path where the dataset is downloaded to as we will use in the torchtitan training Notebook to specify this path as the input channel for the training job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
