{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Llama 3.1 8B Model on Long Context Length (PubMed) Dataset with Context Parallelism and FP8 enabled\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll use the Pubmed dataset to demonstrate how to train a Llama model by enabling long context length distributed training using Amazon SageMaker. We'll compare two approaches: one with context parallelism enabled, and another without it. This comparison will highlight the importance of context parallelism when working with large language models and datasets with long sequences.\n",
    "\n",
    "We also do a comparitive run on p5.48xlarge instances with Context parallelism enabled, but with both FP8 enabled and disabled. This is to demonstrate the incremental throughput benefits we can get by enabling FP8 based training along side context parallelism\n",
    "\n",
    "You can either launch this notebook from an Amazon SageMaker notebook instance which handles all credentials automatically,\n",
    "or by running it locally and setting credentials manually.\n",
    "\n",
    "The notebook is accompanied by the following files:\n",
    "- `train.py`: The entry point script that'll be passed to the SageMaker PyTorch estimator later in this notebook when launching the training job.\n",
    "- `arguments.py`: This file has functions for argument parsing (i.e. hyperparameters).\n",
    "- `checkpoints.py`: This file has functions for saving and loading checkpoints.\n",
    "- `data_utils`: This file has functions for handling S3 URLs.\n",
    "- `data`: This directory has scripts for preparing and loading data.\n",
    "- `fsdp_utils.py`: This file has util functions for fully sharded data parallelism.\n",
    "- `learning_rates.py`: This file has functions for learning rate schedule.\n",
    "- `logging_utils.py`: This file has functions to handle logging.\n",
    "- `memory_tracker.py`: This file has functions to track memory usage.\n",
    "- `requirements.txt`: This file installs the dependencies, including HuggingFace transformers.\n",
    "- `train_lib.py`: This file has functions for running an end-to-end training of the GPT-NeoX or Llama-v2 model with SMP FSDP, settings for hybrid sharding applied, and implemented with code lines to save, load, and fine-tune the model.\n",
    "- `train_utils.py`: This file has utility functions for training.\n",
    "\n",
    "## Additional Resources\n",
    "- To learn more about launching a multi-node distributed PyTorch training job, see [Launching a Distributed Training Job](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#launching-a-distributed-training-job).\n",
    "- To learn more about using the SageMaker Python SDK with PyTorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "- To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "## Prerequisites\n",
    "You need to create an `S3` bucket to store the input data for training.\n",
    "This bucket must be located in the same AWS Region that you choose to launch your training job. To learn how to create a `S3` bucket,\n",
    "see [Create your first S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html) in the Amazon S3 documentation.\n",
    "\n",
    "## Launching Environment\n",
    "\n",
    "### Amazon SageMaker Notebook\n",
    "You can run the notebook with an Amazon SageMaker notebook instance without manually setting your aws credentials.\n",
    "\n",
    "1. Create a new SageMaker notebook instance and open it.\n",
    "2. Zip the contents of this folder & upload to the instance with the `Upload` button on the top-right.\n",
    "3. Open a new terminal with `New -> Terminal`.\n",
    "4. Within the terminal, enter the correct directory and unzip the file.\n",
    "    1. `cd SageMaker && unzip <your-zip-name-here>.zip`\n",
    "\n",
    "### Locally\n",
    "You can run locally by launching a Jupyter notebook server with `jupyter notebook`.\n",
    "This requires you to set your aws credentials in the environment manually.\n",
    "See [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) for more details.\n",
    "\n",
    "## Amazon SageMaker Initialization\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment,\n",
    "such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role.\n",
    "Upgrade SageMaker SDK to the latest version.\n",
    "\n",
    "**NOTE:** This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.233 in /opt/conda/lib/python3.11/site-packages (2.235.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (1.34.162)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (2.2.3)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.15 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (1.0.15)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker>=2.233) (1.26.19)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.162 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2.233) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2.233) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker>=2.233) (0.10.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker>=2.233) (3.20.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (2.10.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (13.9.2)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker>=2.233) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker>=2.233) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker>=2.233) (0.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker>=2.233) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker>=2.233) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker>=2.233) (2024.8.30)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker>=2.233) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker>=2.233) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker>=2.233) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker>=2.233) (2024.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker>=2.233) (1.7.6.9)\n",
      "Collecting dill>=0.3.8 (from pathos->sagemaker>=2.233)\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker>=2.233) (0.3.5)\n",
      "Collecting multiprocess>=0.70.16 (from pathos->sagemaker>=2.233)\n",
      "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (2.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.15->sagemaker>=2.233) (0.1.2)\n",
      "Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Using cached multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
      "Installing collected packages: dill, multiprocess\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.15\n",
      "    Uninstalling multiprocess-0.70.15:\n",
      "      Successfully uninstalled multiprocess-0.70.15\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.14.5 requires dill<0.3.8,>=0.3.0, but you have dill 0.3.9 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.9 multiprocess-0.70.17\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker-experiments in /opt/conda/lib/python3.11/site-packages (0.1.45)\n",
      "Requirement already satisfied: boto3>=1.16.27 in /opt/conda/lib/python3.11/site-packages (from sagemaker-experiments) (1.34.162)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.162 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.34.162)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.16.27->sagemaker-experiments) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.16.27->sagemaker-experiments) (0.10.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.162->boto3>=1.16.27->sagemaker-experiments) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.162->boto3>=1.16.27->sagemaker-experiments) (1.26.19)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.162->boto3>=1.16.27->sagemaker-experiments) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.14.5 in /opt/conda/lib/python3.11/site-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (17.0.0)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.5)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (0.70.17)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets==2.14.5) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (0.25.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets==2.14.5) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.14.5) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.14.5) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.14.5) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.14.5) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets==2.14.5) (1.15.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.5) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.5) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.14.5) (2024.8.30)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.14.5)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==2.14.5) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==2.14.5) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets==2.14.5) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.5) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.0->aiohttp->datasets==2.14.5) (0.2.0)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Installing collected packages: dill, multiprocess\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.17\n",
      "    Uninstalling multiprocess-0.70.17:\n",
      "      Successfully uninstalled multiprocess-0.70.17\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\n",
      "pathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.7 multiprocess-0.70.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade \"sagemaker>=2.233\"\n",
    "%pip install sagemaker-experiments\n",
    "%pip install \"datasets==2.14.5\"\n",
    "%pip install transformers\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Configuration\n",
    "\n",
    "Next, we'll configure our AWS environment and SageMaker session.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "You will need a Huggingface token with access to Llama-3.1-8B-Instruct for this notebook to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "assert hf_token != \"\", \"Please create a Huggingface token and include it above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Execution Role: arn:aws:iam::855988369404:role/CFN-SM-IM-Lambda-Catalog-SageMakerExecutionRole-yrC0QXzmiJdK\n",
      "AWS account: `855988369404`.\n",
      "AWS region: `us-west-2`.\n",
      "\n",
      "Default bucket for this session: `sagemaker-us-west-2-855988369404`.\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: `{account}`.\")\n",
    "\n",
    "# Explicitly set region to us-west-2\n",
    "session = boto3.session.Session(region_name='us-west-2')\n",
    "region = session.region_name\n",
    "print(f\"AWS region: `{region}`.\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\", region_name='us-west-2')  # Also set region here\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(f\"\\nDefault bucket for this session: `{default_bucket}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll load and prepare the PubMed dataset for our experiment.\n",
    "\n",
    "## Pubmed Data Set Attribution\n",
    "\n",
    "Pubmed Dataset is \"Courtesy of the U.S. National Library of Medicine\". This does not indicate the NLM endorses this product or any product AWS builds \n",
    "\n",
    "## PubMed Scientific Papers Dataset Overview\n",
    "The PubMed Scientific Papers dataset is a collection of scientific articles from the biomedical domain, specifically designed for document summarization tasks. Unlike the PubMed abstracts dataset, this contains full scientific papers.\n",
    "\n",
    "## Key Dataset Characteristics\n",
    "- **Size**: ~133,215 articles\n",
    "- **Average Length**: ~6,000 tokens\n",
    "- **Maximum Length**: Can reach 10,000+ words\n",
    "- **Content Type**: Full scientific papers with abstract, introduction, methods, results, and discussion sections\n",
    "\n",
    "## Why It's Ideal for Context Parallelism Demo\n",
    "\n",
    "### 1. Document Length\n",
    "```python\n",
    "# Example token counts\n",
    "average_tokens_per_paper = 6000\n",
    "max_tokens = 16384  # Model's context window\n",
    "\n",
    "# Without CP (single GPU):\n",
    "tokens_per_gpu = 16384\n",
    "\n",
    "# With CP (8 GPUs):\n",
    "tokens_per_gpu = 16384/8  # ~2048 tokens per GPU\n",
    "```\n",
    "\n",
    "### 2. Memory Benefits\n",
    "- **Without CP**: Each GPU handles full 16K sequence\n",
    "- **With CP (degree=8)**:\n",
    "  - Sequences split across GPUs\n",
    "  - Each GPU processes ~2K tokens\n",
    "  - 8x reduction in memory per GPU\n",
    "\n",
    "### 3. Natural Structure\n",
    "- Papers have clear sections\n",
    "- Logical sequence breaks\n",
    "- Each GPU can process semantically coherent chunks\n",
    "- Scientific content benefits from maintaining long-range context\n",
    "\n",
    "### 4. Training Characteristics\n",
    "- Specialized vocabulary\n",
    "- Complex technical content\n",
    "- Long-range dependencies\n",
    "- High information density requiring context preservation\n",
    "\n",
    "### 5. Practical Applications\n",
    "- Scientific document understanding\n",
    "- Long-form content processing\n",
    "- Real-world sequence lengths\n",
    "- Demonstrates CP benefits on actual research content\n",
    "\n",
    "## Memory Requirements Example\n",
    "```python\n",
    "# Assuming hidden_size = 4096\n",
    "hidden_size = 4096\n",
    "seq_len = 16384\n",
    "\n",
    "# Memory per token (simplified)\n",
    "bytes_per_token = 8192 bytes # 2 * hidden_size for BF16\n",
    "\n",
    "# Without CP\n",
    "memory_per_gpu_no_cp = seq_len * bytes_per_token\n",
    "print(f\"Memory per GPU (No CP): {134.22} GB\")\n",
    "\n",
    "# With CP (degree=8)\n",
    "memory_per_gpu_cp = (seq_len/8) * bytes_per_token\n",
    "print(f\"Memory per GPU (CP=8): {16.78} GB\")\n",
    "```\n",
    "\n",
    "This makes the dataset ideal for demonstrating context parallelism's benefits in handling real scientific documents while showing tangible memory savings and processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the PubMed dataset\n",
    "pubmed_dataset = load_dataset(\n",
    "    \"scientific_papers\",\n",
    "    \"pubmed\",\n",
    "    cache_dir=\"/home/ec2-user/SageMaker/datasets\",\n",
    "    download_mode=\"force_redownload\"\n",
    ")\n",
    "\n",
    "# Create a smaller subset of the dataset for our experiment\n",
    "train_test = pubmed_dataset['train'].shuffle(seed=42).select(range(1000)).train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': train_test['test']\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now we'll tokenize our dataset using the Llama tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    "    use_auth_token=hf_token,\n",
    "    cache_dir = \"tmp\"\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['article'])\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = 16384\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data Channels\n",
    "\n",
    "We'll now prepare the data channels for our SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "if lm_datasets[\"train\"] is not None:\n",
    "   train_dataset = lm_datasets[\"train\"]\n",
    "   train_dataset.to_json(\"./training.json\")\n",
    "   training_dataset_location = f\"s3://{default_bucket}/dataset/train/\"\n",
    "   # Extract bucket and key from S3 URI\n",
    "   bucket = default_bucket\n",
    "   key = \"dataset/train/training.json\"\n",
    "   # Upload file using boto3\n",
    "   s3_client.upload_file(\"./training.json\", bucket, key)\n",
    "   # Remove local file\n",
    "   import os\n",
    "   os.remove(\"./training.json\")\n",
    "\n",
    "if lm_datasets[\"validation\"] is not None:\n",
    "   eval_dataset = lm_datasets[\"validation\"]\n",
    "   eval_dataset.to_json(\"./validation.json\")\n",
    "   validation_dataset_location = f\"s3://{default_bucket}/dataset/validation/\"\n",
    "   # Extract bucket and key\n",
    "   bucket = default_bucket  \n",
    "   key = \"dataset/validation/validation.json\"\n",
    "   # Upload file using boto3\n",
    "   s3_client.upload_file(\"./validation.json\", bucket, key)\n",
    "   # Remove local file\n",
    "   os.remove(\"./validation.json\")\n",
    "\n",
    "%store training_dataset_location\n",
    "%store validation_dataset_location\n",
    "\n",
    "s3_train_bucket = training_dataset_location\n",
    "s3_test_bucket = validation_dataset_location\n",
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-fsdp-tp/outputdir/\"\n",
    "\n",
    "if s3_train_bucket != None:\n",
    "   train = sagemaker.inputs.TrainingInput(\n",
    "       s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "   )\n",
    "   data_channels = {\"train\": train}\n",
    "\n",
    "if s3_test_bucket != None:\n",
    "   test = sagemaker.inputs.TrainingInput(\n",
    "       s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "   )\n",
    "   data_channels[\"test\"] = test\n",
    "\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Job without blocking the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_job():\n",
    "    smp_estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Training Parameters\n",
    "\n",
    "Here we define the hyperparameters and configuration for our training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "tensor_parallel_degree = 2  \n",
    "hybrid_shard_degree = 8  \n",
    "save_steps = 10  \n",
    "max_steps = 15  \n",
    "offload_activations = True\n",
    "\n",
    "hyperparameters = {\n",
    "    # Memory and optimization settings\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"auto_wrap_policy\": \"transformer_auto_wrap_policy\",\n",
    "    \"backward_fetch_policy\": \"backward_pre\",\n",
    "    \"clean_cache\": 1,\n",
    "    \"delayed_param\": 1,\n",
    "    \"enable_memory_profiling\": 1,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.95,\n",
    "    \"bf16\": 1,\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.0001,\n",
    "    \"lr_decay_iters\": 47683,\n",
    "    \"lr_decay_style\": \"cosine\",\n",
    "    \"min_lr\": 1e-05,\n",
    "    \"warmup\": 0.0032,\n",
    "    \"weight_decay\": 0.2,\n",
    "    \n",
    "    # Training settings\n",
    "    \"train_batch_size\": 1,\n",
    "    \"val_batch_size\": 1,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"validation_freq\": 5000,\n",
    "    \"validation_batches\": -1,\n",
    "    \"fast_validation\": 0,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"logging_freq\": 1,\n",
    "    \n",
    "    # Checkpoint settings\n",
    "    \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
    "    \"checkpoint_freq\": save_steps,\n",
    "    \"num_kept_checkpoints\": 2,\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model_type\": \"llama_v2\",\n",
    "    \"vocab_size\": 128256,  # Vocab size from Llama 3.1 config file on hugginface\n",
    "    \"num_heads\": 32,\n",
    "    \"num_layers\": 32,\n",
    "    \"intermediate_size\": 14336,\n",
    "    \"hidden_width\": 4096,\n",
    "    \"num_key_value_heads\": 8,\n",
    "    \"llama_intermediate_size\": 14336,\n",
    "    \"hf_pretrained_model_name_or_dir\": model_id,\n",
    "    \n",
    "    # Performance optimization\n",
    "    \"fast_validation\": 0,\n",
    "    \"forward_prefetch\": 1,\n",
    "    \"fp8\": 0,\n",
    "    \"limit_all_gathers\": 1,\n",
    "    \"plateau\": 0.0,\n",
    "    \"seed\": 12345,\n",
    "    \"sharding_strategy\": \"hybrid_shard\",\n",
    "    \"use_smp_flash_attn\": 0,\n",
    "    \"use_smp_implementation\": 1,\n",
    "    \"validation_freq\": save_steps,\n",
    "    \"zipped_data\": 0\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]\n",
    "\n",
    "original_hyperparameters = copy.deepcopy(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without Context Parallelism (P4D.24xLarge)\n",
    "\n",
    "Now, we'll attempt to run the training job without context parallelism to demonstrate why it's necessary for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base job name: `smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-cp8-aoTrue-bs01`.\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "# Instance Settings\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-NON-CP-NOFP8-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "# Parallelism settings\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 0,  # Disable SMP/CP\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1, # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"bf16\": 1,                    # Use BF16\n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"train_batch_size\": 1\n",
    "})\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,  # Enable model parallelism but with minimal parameters\n",
    "                \"parameters\": {\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Non Context Parallelism Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job launched: smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-cp8-aoTrue-bs01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/23/24 00:16:40] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> image_uri is not presented, retrieving image_uri based on            <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#674\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">674</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         instance_type, framework etc.                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/23/24 00:16:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m image_uri is not presented, retrieving image_uri based on            \u001b]8;id=863303;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=323414;file:///opt/conda/lib/python3.11/site-packages/sagemaker/image_uris.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         instance_type, framework etc.                                        \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         smp-8b-NON-CP-NOFP8-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1000</span>-p4d24x-hs8-tp2-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2024</span>-11-23-00-16-39-560        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=508835;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=986093;file:///opt/conda/lib/python3.11/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         smp-8b-NON-CP-NOFP8-\u001b[1;36m1000\u001b[0m-p4d24x-hs8-tp2-\u001b[1;36m2024\u001b[0m-11-23-00-16-39-560        \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-23 00:16:41 Starting - Starting the training job\n",
      "2024-11-23 00:16:41 Pending - Training job waiting for capacity......\n",
      "2024-11-23 00:17:32 Pending - Preparing the instances for training..............................\n",
      "2024-11-23 00:22:55 Downloading - Downloading input data......\n",
      "2024-11-23 00:23:40 Downloading - Downloading the training image..................\n",
      "2024-11-23 00:26:52 Training - Training image download completed. Training in progress....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:21,892 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:22,009 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:22,017 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:22,019 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:22,019 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:23,445 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/einops-0.8.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/flash_attn-2.5.8-py3.11-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mDEPRECATION: Loading egg at /opt/conda/lib/python3.11/site-packages/ninja-1.11.1.1-py3.11-linux-x86_64.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.12.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mCollecting datasets>=2.19.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages/einops-0.8.0-py3.11.egg (from -r requirements.txt (line 3)) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting evaluate (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: expecttest in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (3.12.1)\u001b[0m\n",
      "\u001b[34mCollecting humanize (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.11.0-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting hypothesis (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading hypothesis-6.119.4-py3-none-any.whl.metadata (6.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages/ninja-1.11.1.1-py3.11-linux-x86_64.egg (from -r requirements.txt (line 9)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (2.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers>=4.40.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (4.44.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (2.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.26.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (3.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (2.2.3)\u001b[0m\n",
      "\u001b[34mCollecting requests>=2.32.2 (from datasets>=2.19.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm>=4.66.3 (from datasets>=2.19.1->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.19.1->-r requirements.txt (line 2)) (2024.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=2.19.1->-r requirements.txt (line 2)) (3.10.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from hypothesis->-r requirements.txt (line 8)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 13)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (1.67.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (2.35.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (1.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (75.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (0.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (3.0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.11/site-packages (from tensorboard->-r requirements.txt (line 15)) (0.41.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.40.1->-r requirements.txt (line 16)) (2024.9.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.40.1->-r requirements.txt (line 16)) (0.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 2)) (2.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 2)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 2)) (6.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=2.19.1->-r requirements.txt (line 2)) (1.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (5.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 15)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.1->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.1->-r requirements.txt (line 2)) (3.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.1->-r requirements.txt (line 2)) (2.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets>=2.19.1->-r requirements.txt (line 2)) (2024.8.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (1.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (3.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 15)) (3.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 2)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets>=2.19.1->-r requirements.txt (line 2)) (2024.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 15)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19.1->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 15)) (3.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.19.1->-r requirements.txt (line 2)) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate>=0.12.0->-r requirements.txt (line 1)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\u001b[0m\n",
      "\u001b[34mDownloading humanize-4.11.0-py3-none-any.whl (128 kB)\u001b[0m\n",
      "\u001b[34mDownloading hypothesis-6.119.4-py3-none-any.whl (473 kB)\u001b[0m\n",
      "\u001b[34mDownloading requests-2.32.3-py3-none-any.whl (64 kB)\u001b[0m\n",
      "\u001b[34mDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.67.0-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sortedcontainers, tqdm, requests, hypothesis, humanize, datasets, evaluate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.66.1\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.66.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.66.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: requests\u001b[0m\n",
      "\u001b[34mFound existing installation: requests 2.31.0\u001b[0m\n",
      "\u001b[34mUninstalling requests-2.31.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled requests-2.31.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.19.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker 2.232.3 requires cloudpickle==2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed datasets-3.1.0 evaluate-0.4.3 humanize-4.11.0 hypothesis-6.119.4 requests-2.32.3 sortedcontainers-2.4.0 tqdm-4.67.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,279 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,280 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,424 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,546 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,555 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,670 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-11-23 00:27:26,680 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"activation_checkpointing\": 1,\n",
      "        \"auto_wrap_policy\": \"transformer_auto_wrap_policy\",\n",
      "        \"backward_fetch_policy\": \"backward_pre\",\n",
      "        \"beta1\": 0.9,\n",
      "        \"beta2\": 0.95,\n",
      "        \"bf16\": 1,\n",
      "        \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"checkpoint_freq\": 10,\n",
      "        \"clean_cache\": 0,\n",
      "        \"delayed_param\": 1,\n",
      "        \"do_eval\": false,\n",
      "        \"do_train\": true,\n",
      "        \"enable_memory_profiling\": 1,\n",
      "        \"epochs\": 100,\n",
      "        \"fast_validation\": 0,\n",
      "        \"forward_prefetch\": 1,\n",
      "        \"fp8\": 0,\n",
      "        \"hf_pretrained_model_name_or_dir\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
      "        \"hidden_width\": 4096,\n",
      "        \"intermediate_size\": 14336,\n",
      "        \"limit_all_gathers\": 1,\n",
      "        \"llama_intermediate_size\": 14336,\n",
      "        \"logging_freq\": 1,\n",
      "        \"lr\": 0.0001,\n",
      "        \"lr_decay_iters\": 47683,\n",
      "        \"lr_decay_style\": \"cosine\",\n",
      "        \"max_context_width\": 16384,\n",
      "        \"max_steps\": 15,\n",
      "        \"min_lr\": 1e-05,\n",
      "        \"model_type\": \"llama_v2\",\n",
      "        \"mp_parameters\": {\n",
      "            \"delayed_parameter_initialization\": true,\n",
      "            \"hybrid_shard_degree\": 8\n",
      "        },\n",
      "        \"num_heads\": 32,\n",
      "        \"num_kept_checkpoints\": 2,\n",
      "        \"num_key_value_heads\": 8,\n",
      "        \"num_layers\": 32,\n",
      "        \"offload_activations\": false,\n",
      "        \"plateau\": 0.0,\n",
      "        \"seed\": 12345,\n",
      "        \"sharding_strategy\": \"hybrid_shard\",\n",
      "        \"train_batch_size\": 1,\n",
      "        \"use_smp_flash_attn\": 0,\n",
      "        \"use_smp_implementation\": 0,\n",
      "        \"val_batch_size\": 1,\n",
      "        \"validation_batches\": -1,\n",
      "        \"validation_freq\": 10,\n",
      "        \"vocab_size\": 128256,\n",
      "        \"warmup\": 0.0032,\n",
      "        \"weight_decay\": 0.2,\n",
      "        \"zipped_data\": 0\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-2024-11-23-00-16-39-560\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-855988369404/smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-2024-11-23-00-16-39-560/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"activation_checkpointing\":1,\"auto_wrap_policy\":\"transformer_auto_wrap_policy\",\"backward_fetch_policy\":\"backward_pre\",\"beta1\":0.9,\"beta2\":0.95,\"bf16\":1,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"checkpoint_freq\":10,\"clean_cache\":0,\"delayed_param\":1,\"do_eval\":false,\"do_train\":true,\"enable_memory_profiling\":1,\"epochs\":100,\"fast_validation\":0,\"forward_prefetch\":1,\"fp8\":0,\"hf_pretrained_model_name_or_dir\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"hidden_width\":4096,\"intermediate_size\":14336,\"limit_all_gathers\":1,\"llama_intermediate_size\":14336,\"logging_freq\":1,\"lr\":0.0001,\"lr_decay_iters\":47683,\"lr_decay_style\":\"cosine\",\"max_context_width\":16384,\"max_steps\":15,\"min_lr\":1e-05,\"model_type\":\"llama_v2\",\"mp_parameters\":{\"delayed_parameter_initialization\":true,\"hybrid_shard_degree\":8},\"num_heads\":32,\"num_kept_checkpoints\":2,\"num_key_value_heads\":8,\"num_layers\":32,\"offload_activations\":false,\"plateau\":0.0,\"seed\":12345,\"sharding_strategy\":\"hybrid_shard\",\"train_batch_size\":1,\"use_smp_flash_attn\":0,\"use_smp_implementation\":0,\"val_batch_size\":1,\"validation_batches\":-1,\"validation_freq\":10,\"vocab_size\":128256,\"warmup\":0.0032,\"weight_decay\":0.2,\"zipped_data\":0}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-855988369404/smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-2024-11-23-00-16-39-560/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"activation_checkpointing\":1,\"auto_wrap_policy\":\"transformer_auto_wrap_policy\",\"backward_fetch_policy\":\"backward_pre\",\"beta1\":0.9,\"beta2\":0.95,\"bf16\":1,\"checkpoint_dir\":\"/opt/ml/checkpoints\",\"checkpoint_freq\":10,\"clean_cache\":0,\"delayed_param\":1,\"do_eval\":false,\"do_train\":true,\"enable_memory_profiling\":1,\"epochs\":100,\"fast_validation\":0,\"forward_prefetch\":1,\"fp8\":0,\"hf_pretrained_model_name_or_dir\":\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"hidden_width\":4096,\"intermediate_size\":14336,\"limit_all_gathers\":1,\"llama_intermediate_size\":14336,\"logging_freq\":1,\"lr\":0.0001,\"lr_decay_iters\":47683,\"lr_decay_style\":\"cosine\",\"max_context_width\":16384,\"max_steps\":15,\"min_lr\":1e-05,\"model_type\":\"llama_v2\",\"mp_parameters\":{\"delayed_parameter_initialization\":true,\"hybrid_shard_degree\":8},\"num_heads\":32,\"num_kept_checkpoints\":2,\"num_key_value_heads\":8,\"num_layers\":32,\"offload_activations\":false,\"plateau\":0.0,\"seed\":12345,\"sharding_strategy\":\"hybrid_shard\",\"train_batch_size\":1,\"use_smp_flash_attn\":0,\"use_smp_implementation\":0,\"val_batch_size\":1,\"validation_batches\":-1,\"validation_freq\":10,\"vocab_size\":128256,\"warmup\":0.0032,\"weight_decay\":0.2,\"zipped_data\":0},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-2024-11-23-00-16-39-560\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-855988369404/smp-8b-NON-CP-NOFP8-1000-p4d24x-hs8-tp2-2024-11-23-00-16-39-560/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--activation_checkpointing\",\"1\",\"--auto_wrap_policy\",\"transformer_auto_wrap_policy\",\"--backward_fetch_policy\",\"backward_pre\",\"--beta1\",\"0.9\",\"--beta2\",\"0.95\",\"--bf16\",\"1\",\"--checkpoint_dir\",\"/opt/ml/checkpoints\",\"--checkpoint_freq\",\"10\",\"--clean_cache\",\"0\",\"--delayed_param\",\"1\",\"--do_eval\",\"False\",\"--do_train\",\"True\",\"--enable_memory_profiling\",\"1\",\"--epochs\",\"100\",\"--fast_validation\",\"0\",\"--forward_prefetch\",\"1\",\"--fp8\",\"0\",\"--hf_pretrained_model_name_or_dir\",\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\"--hidden_width\",\"4096\",\"--intermediate_size\",\"14336\",\"--limit_all_gathers\",\"1\",\"--llama_intermediate_size\",\"14336\",\"--logging_freq\",\"1\",\"--lr\",\"0.0001\",\"--lr_decay_iters\",\"47683\",\"--lr_decay_style\",\"cosine\",\"--max_context_width\",\"16384\",\"--max_steps\",\"15\",\"--min_lr\",\"1e-05\",\"--model_type\",\"llama_v2\",\"--mp_parameters\",\"delayed_parameter_initialization=True,hybrid_shard_degree=8\",\"--num_heads\",\"32\",\"--num_kept_checkpoints\",\"2\",\"--num_key_value_heads\",\"8\",\"--num_layers\",\"32\",\"--offload_activations\",\"False\",\"--plateau\",\"0.0\",\"--seed\",\"12345\",\"--sharding_strategy\",\"hybrid_shard\",\"--train_batch_size\",\"1\",\"--use_smp_flash_attn\",\"0\",\"--use_smp_implementation\",\"0\",\"--val_batch_size\",\"1\",\"--validation_batches\",\"-1\",\"--validation_freq\",\"10\",\"--vocab_size\",\"128256\",\"--warmup\",\"0.0032\",\"--weight_decay\",\"0.2\",\"--zipped_data\",\"0\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ACTIVATION_CHECKPOINTING=1\u001b[0m\n",
      "\u001b[34mSM_HP_AUTO_WRAP_POLICY=transformer_auto_wrap_policy\u001b[0m\n",
      "\u001b[34mSM_HP_BACKWARD_FETCH_POLICY=backward_pre\u001b[0m\n",
      "\u001b[34mSM_HP_BETA1=0.9\u001b[0m\n",
      "\u001b[34mSM_HP_BETA2=0.95\u001b[0m\n",
      "\u001b[34mSM_HP_BF16=1\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_CHECKPOINT_FREQ=10\u001b[0m\n",
      "\u001b[34mSM_HP_CLEAN_CACHE=0\u001b[0m\n",
      "\u001b[34mSM_HP_DELAYED_PARAM=1\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=false\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_MEMORY_PROFILING=1\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=100\u001b[0m\n",
      "\u001b[34mSM_HP_FAST_VALIDATION=0\u001b[0m\n",
      "\u001b[34mSM_HP_FORWARD_PREFETCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_FP8=0\u001b[0m\n",
      "\u001b[34mSM_HP_HF_PRETRAINED_MODEL_NAME_OR_DIR=meta-llama/Meta-Llama-3.1-8B-Instruct\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_WIDTH=4096\u001b[0m\n",
      "\u001b[34mSM_HP_INTERMEDIATE_SIZE=14336\u001b[0m\n",
      "\u001b[34mSM_HP_LIMIT_ALL_GATHERS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LLAMA_INTERMEDIATE_SIZE=14336\u001b[0m\n",
      "\u001b[34mSM_HP_LOGGING_FREQ=1\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LR_DECAY_ITERS=47683\u001b[0m\n",
      "\u001b[34mSM_HP_LR_DECAY_STYLE=cosine\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_CONTEXT_WIDTH=16384\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=15\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_LR=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=llama_v2\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"delayed_parameter_initialization\":true,\"hybrid_shard_degree\":8}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_HEADS=32\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_KEPT_CHECKPOINTS=2\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_KEY_VALUE_HEADS=8\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LAYERS=32\u001b[0m\n",
      "\u001b[34mSM_HP_OFFLOAD_ACTIVATIONS=false\u001b[0m\n",
      "\u001b[34mSM_HP_PLATEAU=0.0\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=12345\u001b[0m\n",
      "\u001b[34mSM_HP_SHARDING_STRATEGY=hybrid_shard\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_USE_SMP_FLASH_ATTN=0\u001b[0m\n",
      "\u001b[34mSM_HP_USE_SMP_IMPLEMENTATION=0\u001b[0m\n",
      "\u001b[34mSM_HP_VAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_BATCHES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FREQ=10\u001b[0m\n",
      "\u001b[34mSM_HP_VOCAB_SIZE=128256\u001b[0m\n",
      "\u001b[34mSM_HP_WARMUP=0.0032\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_ZIPPED_DATA=0\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages:/opt/conda/lib/python3.11/site-packages/flash_attn-2.5.8-py3.11-linux-x86_64.egg:/opt/conda/lib/python3.11/site-packages/ninja-1.11.1.1-py3.11-linux-x86_64.egg:/opt/conda/lib/python3.11/site-packages/einops-0.8.0-py3.11.egg:/opt/conda/lib/python3.11/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 8 train.py --activation_checkpointing 1 --auto_wrap_policy transformer_auto_wrap_policy --backward_fetch_policy backward_pre --beta1 0.9 --beta2 0.95 --bf16 1 --checkpoint_dir /opt/ml/checkpoints --checkpoint_freq 10 --clean_cache 0 --delayed_param 1 --do_eval False --do_train True --enable_memory_profiling 1 --epochs 100 --fast_validation 0 --forward_prefetch 1 --fp8 0 --hf_pretrained_model_name_or_dir meta-llama/Meta-Llama-3.1-8B-Instruct --hidden_width 4096 --intermediate_size 14336 --limit_all_gathers 1 --llama_intermediate_size 14336 --logging_freq 1 --lr 0.0001 --lr_decay_iters 47683 --lr_decay_style cosine --max_context_width 16384 --max_steps 15 --min_lr 1e-05 --model_type llama_v2 --mp_parameters delayed_parameter_initialization=True,hybrid_shard_degree=8 --num_heads 32 --num_kept_checkpoints 2 --num_key_value_heads 8 --num_layers 32 --offload_activations False --plateau 0.0 --seed 12345 --sharding_strategy hybrid_shard --train_batch_size 1 --use_smp_flash_attn 0 --use_smp_implementation 0 --val_batch_size 1 --validation_batches -1 --validation_freq 10 --vocab_size 128256 --warmup 0.0032 --weight_decay 0.2 --zipped_data 0\u001b[0m\n",
      "\u001b[34mW1123 00:27:27.906000 139798496560960 torch/distributed/run.py:779] \u001b[0m\n",
      "\u001b[34mW1123 00:27:27.906000 139798496560960 torch/distributed/run.py:779] *****************************************\u001b[0m\n",
      "\u001b[34mW1123 00:27:27.906000 139798496560960 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34mW1123 00:27:27.906000 139798496560960 torch/distributed/run.py:779] *****************************************\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.388: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.388: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.438: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.455: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.458: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:473: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:497: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:521: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:527: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:536: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:542: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:549: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:558: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:565: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:570: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:581: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/utils.py:600: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.472: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.475: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m[2024-11-23 00:27:30.487: W torch/sagemaker/state_handler.py:46] Disabling Torch compile for using torch.sagemaker\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:278: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:294: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:389: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/tensor_parallel/layers.py:428: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/megatron/core/dist_checkpointing/strategies/torch.py:22: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "  from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:193: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.11/site-packages/torch/sagemaker/tensor_parallel/embedding.py:258: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Context Parallelism (P4D.24XLarge)\n",
    "\n",
    "In this section, we set up and run the training job with context parallelism enabled. This configuration should successfully handle the large model and long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_hyperparameters = copy.deepcopy(hyperparameters)\n",
    "\n",
    "# Instance Settings\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-CP-NOFP8-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 1,  # Enable SMP/CP\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1, # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"bf16\": 1,                    # Use BF16 \n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"train_batch_size\": 1\n",
    "})\n",
    "\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"context_parallel_degree\": context_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Context Parallelism Enabled Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcasing the value of FP8 enabled training using Context Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP8, a datatype supported by NVIDIA's H100 and H200 GPUs, has revolutionized deep learning workloads with its remarkable efficiency. This innovative format occupies a mere 8 bits of memory, half that of its bf16 or fp16 counterparts, significantly reducing computational costs for operations like matrix multiplication.\n",
    "\n",
    "The next few sections are going to show you how we can increase the speed of our training using FP8 enabled Sagemaker Training Jobs. We will compare two jobs for their epoch speed, both with context parallelism enabled but one with and another without FP8 enabled. Both of these will be run on a P5.48xlarge instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Context Parallelism WITH FP8 for increased training throughput (P5.48xlarge)\n",
    "\n",
    "Modern training of large language models (LLMs) combines two key optimizations:\n",
    "- **Context Parallelism (CP)**: Distributes long sequences across GPUs\n",
    "- **FP8 Training**: Uses 8-bit precision for computations and activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 1,  # Enable SMP/CP\n",
    "    \"train_batch_size\": 4,  # Train Batch Size 4\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1,  # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"fp8\": 1,  # Enable FP8 flag\n",
    "    \"distributed_backend\": \"nccl\"  # Add this line to explicitly use NCCL\n",
    "\n",
    "})\n",
    "\n",
    "# Instance Settings\n",
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-CP-WITH-FP8-TBS-4-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"context_parallel_degree\": context_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Context Parallelism with FP8 Enabled Job (ON P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Context Parallelism WITHOUT FP8 (P5.48xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 1,  # Enable SMP/CP\n",
    "    \"train_batch_size\": 4,  # Train Batch Size 4\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1, # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"bf16\": 1,                    # Use BF16 \n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"distributed_backend\": \"nccl\"  # Add this line to explicitly use NCCL\n",
    "\n",
    "})\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "# Instance Settings\n",
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-CP-NOFP8-TBS-4-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"context_parallel_degree\": context_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Context Parallelism without FP8 Enabled Job (ON P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the process of setting up and running training jobs for the PubMed dataset using the Llama model, both with and without context parallelism. \n",
    "\n",
    "Key observations:\n",
    "\n",
    "1. With context parallelism enabled, the training job runs successfully. This is because context parallelism allows for efficient distribution of the model across multiple GPUs, reducing the memory requirements per GPU.\n",
    "\n",
    "![With Context Parallelism](./with_cp.png)\n",
    "\n",
    "2. Without context parallelism, the training job fails. This is likely due to memory constraints, as the large Llama model cannot fit into a single GPU's memory when processing long sequences from the PubMed dataset.\n",
    "\n",
    "![Without Context Parallelism](./without_cp.png)\n",
    "\n",
    "\n",
    "This experiment highlights the importance of techniques like context parallelism when working with large language models and datasets with long sequences. It allows us to train models that would otherwise be too large to fit in memory, enabling work with more complex models and larger datasets.\n",
    "\n",
    "Context Parallelism with FP8 enabled vs not enabled (On P5)\n",
    "\n",
    "1. **With FP8 Enabled** :\n",
    "\n",
    "![With FP8](./With_FP8.png)\n",
    "\n",
    "2. **Without FP8 Enabled**:\n",
    "\n",
    "![Without FP8](./WO_FP8_P5.png)\n",
    "\n",
    "If you look at the TFLOPS / GPU metric, We can clearly see that with FP8 the throughput is higher\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
