{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Llama 3.1 8B Model on Long Context Length (PubMed) Dataset with Context Parallelism and FP8 enabled\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we'll use the Pubmed dataset to demonstrate how to train a Llama model by enabling long context length distributed training using Amazon SageMaker. We'll compare two approaches: one with context parallelism enabled, and another without it. This comparison will highlight the importance of context parallelism when working with large language models and datasets with long sequences.\n",
    "\n",
    "We also do a comparitive run on p5.48xlarge instances with Context parallelism enabled, but with both FP8 enabled and disabled. This is to demonstrate the incremental throughput benefits we can get by enabling FP8 based training along side context parallelism\n",
    "\n",
    "You can either launch this notebook from an Amazon SageMaker notebook instance which handles all credentials automatically,\n",
    "or by running it locally and setting credentials manually.\n",
    "\n",
    "The notebook is accompanied by the following files:\n",
    "- `train.py`: The entry point script that'll be passed to the SageMaker PyTorch estimator later in this notebook when launching the training job.\n",
    "- `arguments.py`: This file has functions for argument parsing (i.e. hyperparameters).\n",
    "- `checkpoints.py`: This file has functions for saving and loading checkpoints.\n",
    "- `data_utils`: This file has functions for handling S3 URLs.\n",
    "- `data`: This directory has scripts for preparing and loading data.\n",
    "- `fsdp_utils.py`: This file has util functions for fully sharded data parallelism.\n",
    "- `learning_rates.py`: This file has functions for learning rate schedule.\n",
    "- `logging_utils.py`: This file has functions to handle logging.\n",
    "- `memory_tracker.py`: This file has functions to track memory usage.\n",
    "- `requirements.txt`: This file installs the dependencies, including HuggingFace transformers.\n",
    "- `train_lib.py`: This file has functions for running an end-to-end training of the GPT-NeoX or Llama-v2 model with SMP FSDP, settings for hybrid sharding applied, and implemented with code lines to save, load, and fine-tune the model.\n",
    "- `train_utils.py`: This file has utility functions for training.\n",
    "\n",
    "## Additional Resources\n",
    "- To learn more about launching a multi-node distributed PyTorch training job, see [Launching a Distributed Training Job](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#launching-a-distributed-training-job).\n",
    "- To learn more about using the SageMaker Python SDK with PyTorch, see [Using PyTorch with the SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html).\n",
    "- To learn more about launching a training job in Amazon SageMaker with your own training image, see [Use Your Own Training Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html).\n",
    "\n",
    "## Prerequisites\n",
    "You need to create an `S3` bucket to store the input data for training.\n",
    "This bucket must be located in the same AWS Region that you choose to launch your training job. To learn how to create a `S3` bucket,\n",
    "see [Create your first S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html) in the Amazon S3 documentation.\n",
    "\n",
    "## Launching Environment\n",
    "\n",
    "### Amazon SageMaker Notebook\n",
    "You can run the notebook with an Amazon SageMaker notebook instance without manually setting your aws credentials.\n",
    "\n",
    "1. Create a new SageMaker notebook instance and open it.\n",
    "2. Zip the contents of this folder & upload to the instance with the `Upload` button on the top-right.\n",
    "3. Open a new terminal with `New -> Terminal`.\n",
    "4. Within the terminal, enter the correct directory and unzip the file.\n",
    "    1. `cd SageMaker && unzip <your-zip-name-here>.zip`\n",
    "\n",
    "### Locally\n",
    "You can run locally by launching a Jupyter notebook server with `jupyter notebook`.\n",
    "This requires you to set your aws credentials in the environment manually.\n",
    "See [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) for more details.\n",
    "\n",
    "## Amazon SageMaker Initialization\n",
    "Run the following cell to import SageMaker modules and retrieve information of your current SageMaker work environment,\n",
    "such as your AWS account ID, the AWS Region, and the ARN of your Amazon SageMaker execution role.\n",
    "Upgrade SageMaker SDK to the latest version.\n",
    "\n",
    "**NOTE:** This step might require a kernel restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, we'll set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade \"sagemaker>=2.233\"\n",
    "%pip install sagemaker-experiments\n",
    "%pip install \"datasets==2.14.5\"\n",
    "%pip install transformers\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import threading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Configuration\n",
    "\n",
    "Next, we'll configure our AWS environment and SageMaker session.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "You will need a Huggingface token with access to Llama-3.1-8B-Instruct for this notebook to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "assert hf_token != \"\", \"Please create a Huggingface token and include it above\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "client = boto3.client(\"sts\")\n",
    "account = client.get_caller_identity()[\"Account\"]\n",
    "print(f\"AWS account: `{account}`.\")\n",
    "\n",
    "# Explicitly set region to us-west-2\n",
    "session = boto3.session.Session(region_name='us-west-2')\n",
    "region = session.region_name\n",
    "print(f\"AWS region: `{region}`.\")\n",
    "\n",
    "sm_boto_client = boto3.client(\"sagemaker\", region_name='us-west-2')  # Also set region here\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(f\"\\nDefault bucket for this session: `{default_bucket}`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll load and prepare the PubMed dataset for our experiment.\n",
    "\n",
    "## Pubmed Data Set Attribution\n",
    "\n",
    "Pubmed Dataset is \"Courtesy of the U.S. National Library of Medicine\". This does not indicate the NLM endorses this product or any product AWS builds \n",
    "\n",
    "## PubMed Scientific Papers Dataset Overview\n",
    "The PubMed Scientific Papers dataset is a collection of scientific articles from the biomedical domain, specifically designed for document summarization tasks. Unlike the PubMed abstracts dataset, this contains full scientific papers.\n",
    "\n",
    "## Key Dataset Characteristics\n",
    "- **Size**: ~133,215 articles\n",
    "- **Average Length**: ~6,000 tokens\n",
    "- **Maximum Length**: Can reach 10,000+ words\n",
    "- **Content Type**: Full scientific papers with abstract, introduction, methods, results, and discussion sections\n",
    "\n",
    "## Why It's Ideal for Context Parallelism Demo\n",
    "\n",
    "### 1. Document Length\n",
    "```python\n",
    "# Example token counts\n",
    "average_tokens_per_paper = 6000\n",
    "max_tokens = 16384  # Model's context window\n",
    "\n",
    "# Without CP (single GPU):\n",
    "tokens_per_gpu = 16384\n",
    "\n",
    "# With CP (8 GPUs):\n",
    "tokens_per_gpu = 16384/8  # ~2048 tokens per GPU\n",
    "```\n",
    "\n",
    "### 2. Memory Benefits\n",
    "- **Without CP**: Each GPU handles full 16K sequence\n",
    "- **With CP (degree=8)**:\n",
    "  - Sequences split across GPUs\n",
    "  - Each GPU processes ~2K tokens\n",
    "  - 8x reduction in memory per GPU\n",
    "\n",
    "### 3. Natural Structure\n",
    "- Papers have clear sections\n",
    "- Logical sequence breaks\n",
    "- Each GPU can process semantically coherent chunks\n",
    "- Scientific content benefits from maintaining long-range context\n",
    "\n",
    "### 4. Training Characteristics\n",
    "- Specialized vocabulary\n",
    "- Complex technical content\n",
    "- Long-range dependencies\n",
    "- High information density requiring context preservation\n",
    "\n",
    "### 5. Practical Applications\n",
    "- Scientific document understanding\n",
    "- Long-form content processing\n",
    "- Real-world sequence lengths\n",
    "- Demonstrates CP benefits on actual research content\n",
    "\n",
    "## Memory Requirements Example\n",
    "```python\n",
    "# Assuming hidden_size = 4096\n",
    "hidden_size = 4096\n",
    "seq_len = 16384\n",
    "\n",
    "# Memory per token (simplified)\n",
    "bytes_per_token = 8192 bytes # 2 * hidden_size for BF16\n",
    "\n",
    "# Without CP\n",
    "memory_per_gpu_no_cp = seq_len * bytes_per_token\n",
    "print(f\"Memory per GPU (No CP): {134.22} GB\")\n",
    "\n",
    "# With CP (degree=8)\n",
    "memory_per_gpu_cp = (seq_len/8) * bytes_per_token\n",
    "print(f\"Memory per GPU (CP=8): {16.78} GB\")\n",
    "```\n",
    "\n",
    "This makes the dataset ideal for demonstrating context parallelism's benefits in handling real scientific documents while showing tangible memory savings and processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the PubMed dataset\n",
    "pubmed_dataset = load_dataset(\n",
    "    \"scientific_papers\",\n",
    "    \"pubmed\",\n",
    "    cache_dir=\"/home/ec2-user/SageMaker/datasets\",\n",
    "    download_mode=\"force_redownload\"\n",
    ")\n",
    "\n",
    "# Create a smaller subset of the dataset for our experiment\n",
    "train_test = pubmed_dataset['train'].shuffle(seed=42).select(range(1000)).train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'validation': train_test['test']\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Now we'll tokenize our dataset using the Llama tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=True,\n",
    "    use_auth_token=hf_token,\n",
    "    cache_dir = \"tmp\"\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['article'])\n",
    "\n",
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "block_size = 16384\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data Channels\n",
    "\n",
    "We'll now prepare the data channels for our SageMaker training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "if lm_datasets[\"train\"] is not None:\n",
    "   train_dataset = lm_datasets[\"train\"]\n",
    "   train_dataset.to_json(\"./training.json\")\n",
    "   training_dataset_location = f\"s3://{default_bucket}/dataset/train/\"\n",
    "   # Extract bucket and key from S3 URI\n",
    "   bucket = default_bucket\n",
    "   key = \"dataset/train/training.json\"\n",
    "   # Upload file using boto3\n",
    "   s3_client.upload_file(\"./training.json\", bucket, key)\n",
    "   # Remove local file\n",
    "   import os\n",
    "   os.remove(\"./training.json\")\n",
    "\n",
    "if lm_datasets[\"validation\"] is not None:\n",
    "   eval_dataset = lm_datasets[\"validation\"]\n",
    "   eval_dataset.to_json(\"./validation.json\")\n",
    "   validation_dataset_location = f\"s3://{default_bucket}/dataset/validation/\"\n",
    "   # Extract bucket and key\n",
    "   bucket = default_bucket  \n",
    "   key = \"dataset/validation/validation.json\"\n",
    "   # Upload file using boto3\n",
    "   s3_client.upload_file(\"./validation.json\", bucket, key)\n",
    "   # Remove local file\n",
    "   os.remove(\"./validation.json\")\n",
    "\n",
    "%store training_dataset_location\n",
    "%store validation_dataset_location\n",
    "\n",
    "s3_train_bucket = training_dataset_location\n",
    "s3_test_bucket = validation_dataset_location\n",
    "s3_output_bucket = f\"s3://sagemaker-{region}-{account}/smp-fsdp-tp/outputdir/\"\n",
    "\n",
    "if s3_train_bucket != None:\n",
    "   train = sagemaker.inputs.TrainingInput(\n",
    "       s3_train_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "   )\n",
    "   data_channels = {\"train\": train}\n",
    "\n",
    "if s3_test_bucket != None:\n",
    "   test = sagemaker.inputs.TrainingInput(\n",
    "       s3_test_bucket, distribution=\"FullyReplicated\", s3_data_type=\"S3Prefix\"\n",
    "   )\n",
    "   data_channels[\"test\"] = test\n",
    "\n",
    "print(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Job without blocking the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_job():\n",
    "    smp_estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Training Parameters\n",
    "\n",
    "Here we define the hyperparameters and configuration for our training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "tensor_parallel_degree = 2  \n",
    "hybrid_shard_degree = 8  \n",
    "save_steps = 10  \n",
    "max_steps = 15  \n",
    "offload_activations = True\n",
    "\n",
    "hyperparameters = {\n",
    "    # Memory and optimization settings\n",
    "    \"activation_checkpointing\": 1,\n",
    "    \"auto_wrap_policy\": \"transformer_auto_wrap_policy\",\n",
    "    \"backward_fetch_policy\": \"backward_pre\",\n",
    "    \"clean_cache\": 1,\n",
    "    \"delayed_param\": 1,\n",
    "    \"enable_memory_profiling\": 1,\n",
    "    \n",
    "    # Training parameters\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.95,\n",
    "    \"bf16\": 1,\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 0.0001,\n",
    "    \"lr_decay_iters\": 47683,\n",
    "    \"lr_decay_style\": \"cosine\",\n",
    "    \"min_lr\": 1e-05,\n",
    "    \"warmup\": 0.0032,\n",
    "    \"weight_decay\": 0.2,\n",
    "    \n",
    "    # Training settings\n",
    "    \"train_batch_size\": 1,\n",
    "    \"val_batch_size\": 1,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"validation_freq\": 5000,\n",
    "    \"validation_batches\": -1,\n",
    "    \"fast_validation\": 0,\n",
    "    \"max_steps\": max_steps,\n",
    "    \"logging_freq\": 1,\n",
    "    \n",
    "    # Checkpoint settings\n",
    "    \"checkpoint_dir\": \"/opt/ml/checkpoints\",\n",
    "    \"checkpoint_freq\": save_steps,\n",
    "    \"num_kept_checkpoints\": 2,\n",
    "    \n",
    "    # Model configuration\n",
    "    \"model_type\": \"llama_v2\",\n",
    "    \"vocab_size\": 128256,  # Vocab size from Llama 3.1 config file on hugginface\n",
    "    \"num_heads\": 32,\n",
    "    \"num_layers\": 32,\n",
    "    \"intermediate_size\": 14336,\n",
    "    \"hidden_width\": 4096,\n",
    "    \"num_key_value_heads\": 8,\n",
    "    \"llama_intermediate_size\": 14336,\n",
    "    \"hf_pretrained_model_name_or_dir\": model_id,\n",
    "    \n",
    "    # Performance optimization\n",
    "    \"fast_validation\": 0,\n",
    "    \"forward_prefetch\": 1,\n",
    "    \"fp8\": 0,\n",
    "    \"limit_all_gathers\": 1,\n",
    "    \"plateau\": 0.0,\n",
    "    \"seed\": 12345,\n",
    "    \"sharding_strategy\": \"hybrid_shard\",\n",
    "    \"use_smp_flash_attn\": 0,\n",
    "    \"use_smp_implementation\": 1,\n",
    "    \"validation_freq\": save_steps,\n",
    "    \"zipped_data\": 0\n",
    "}\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"base_metric\", \"Regex\": \"<><><><><><>\"}\n",
    "]\n",
    "\n",
    "original_hyperparameters = copy.deepcopy(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training without Context Parallelism (P4D.24xLarge)\n",
    "\n",
    "Now, we'll attempt to run the training job without context parallelism to demonstrate why it's necessary for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "# Instance Settings\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-NON-CP-NOFP8-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "# Parallelism settings\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 0,  # Disable SMP/CP\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1, # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"bf16\": 1,                    # Use BF16\n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"train_batch_size\": 1\n",
    "})\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,  # Enable model parallelism but with minimal parameters\n",
    "                \"parameters\": {\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Non Context Parallelism Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Context Parallelism (P4D.24XLarge)\n",
    "\n",
    "In this section, we set up and run the training job with context parallelism enabled. This configuration should successfully handle the large model and long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "# Instance Settings\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-CP-NOFP8-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 1,  # Enable SMP/CP\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1, # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"bf16\": 1,                    # Use BF16 \n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"train_batch_size\": 1\n",
    "})\n",
    "\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"context_parallel_degree\": context_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Context Parallelism Enabled Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcasing the value of FP8 enabled training using Context Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP8, a datatype supported by NVIDIA's H100 and H200 GPUs, has revolutionized deep learning workloads with its remarkable efficiency. This innovative format occupies a mere 8 bits of memory, half that of its bf16 or fp16 counterparts, significantly reducing computational costs for operations like matrix multiplication.\n",
    "\n",
    "The next few sections are going to show you how we can increase the speed of our training using FP8 enabled Sagemaker Training Jobs. We will compare two jobs for their epoch speed, both with context parallelism enabled but one with and another without FP8 enabled. Both of these will be run on a P5.48xlarge instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Context Parallelism WITH FP8 for increased training throughput (P5.48xlarge)\n",
    "\n",
    "Modern training of large language models (LLMs) combines two key optimizations:\n",
    "- **Context Parallelism (CP)**: Distributes long sequences across GPUs\n",
    "- **FP8 Training**: Uses 8-bit precision for computations and activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 1,  # Enable SMP/CP\n",
    "    \"train_batch_size\": 4,  # Train Batch Size 4\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1,  # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"fp8\": 1,  # Enable FP8 flag\n",
    "    \"distributed_backend\": \"nccl\"  # Add this line to explicitly use NCCL\n",
    "\n",
    "})\n",
    "\n",
    "# Instance Settings\n",
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-CP-WITH-FP8-TBS-4-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"context_parallel_degree\": context_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Context Parallelism with FP8 Enabled Job (ON P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Context Parallelism WITHOUT FP8 (P5.48xlarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = copy.deepcopy(original_hyperparameters)\n",
    "\n",
    "\n",
    "hyperparameters.update({\n",
    "    \"use_smp_implementation\": 1,  # Enable SMP/CP\n",
    "    \"train_batch_size\": 4,  # Train Batch Size 4\n",
    "    \"max_context_width\": 16384,   # Full sequence length\n",
    "    \"activation_checkpointing\": 1, # Disable activation checkpointing\n",
    "    \"clean_cache\": 0,\n",
    "    \"bf16\": 1,                    # Use BF16 \n",
    "    \"offload_activations\": False,\n",
    "    \"use_smp_flash_attn\": 0,       # Disable flash attention\n",
    "    \"distributed_backend\": \"nccl\"  # Add this line to explicitly use NCCL\n",
    "\n",
    "})\n",
    "\n",
    "# Parallelism settings\n",
    "context_parallel_degree = 8\n",
    "hybrid_shard_degree = 8  \n",
    "\n",
    "# Instance Settings\n",
    "instance_type = \"ml.p5.48xlarge\"\n",
    "instance_count = 1\n",
    "processes_per_host = 8\n",
    "\n",
    "instance_type_str = instance_type.split(\".\")[1] + instance_type.split(\".\")[2][:3]\n",
    "base_job_name = f'smp-8b-CP-NOFP8-TBS-4-1000-{instance_type_str}-hs{hybrid_shard_degree}-tp{tensor_parallel_degree}-cp{context_parallel_degree}-ao{offload_activations}-bs{hyperparameters[\"train_batch_size\"]:02d}'\n",
    "print(f\"Base job name: `{base_job_name}`.\")\n",
    "\n",
    "checkpoint_bucket = f\"s3://sagemaker-{region}-{account}\"\n",
    "checkpoint_s3_uri = f\"{checkpoint_bucket}/experiments/smp-fsdp-tp-llama_v2-checkpoints/{base_job_name}/\"\n",
    "\n",
    "smp_estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    source_dir=os.path.join(os.getcwd(), \"./shared-scripts\"),\n",
    "    role=role,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    instance_type=instance_type,\n",
    "    volume_size=400,\n",
    "    instance_count=instance_count,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\n",
    "        \"torch_distributed\": {\n",
    "            \"enabled\": True,\n",
    "        },\n",
    "        \"smdistributed\": {\n",
    "            \"modelparallel\": {\n",
    "                \"enabled\": True,\n",
    "                \"parameters\": {\n",
    "                    \"context_parallel_degree\": context_parallel_degree,\n",
    "                    \"hybrid_shard_degree\": hybrid_shard_degree,\n",
    "                    \"delayed_parameter_initialization\": True,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    py_version=\"py311\",\n",
    "    framework_version=\"2.4.1\",\n",
    "    output_path=s3_output_bucket,\n",
    "    max_run=86400,\n",
    "    debugger_hook_config=False,\n",
    "    base_job_name=base_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    environment={\n",
    "        \"HF_TOKEN\": hf_token,\n",
    "        \"NCCL_DEBUG\": \"INFO\",\n",
    "        \"NCCL_MIN_NRINGS\": \"1\",\n",
    "        \"NCCL_IB_TIMEOUT\": \"22\"\n",
    "    }, \n",
    "    wait = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Context Parallelism without FP8 Enabled Job (ON P5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = threading.Thread(target=launch_job)\n",
    "thread.daemon = True  # Allow the thread to be terminated when notebook closes\n",
    "thread.start()\n",
    "print(f\"Job launched: {base_job_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated the process of setting up and running training jobs for the PubMed dataset using the Llama model, both with and without context parallelism. \n",
    "\n",
    "Key observations:\n",
    "\n",
    "1. With context parallelism enabled, the training job runs successfully. This is because context parallelism allows for efficient distribution of the model across multiple GPUs, reducing the memory requirements per GPU.\n",
    "\n",
    "![With Context Parallelism](./with_cp.png)\n",
    "\n",
    "2. Without context parallelism, the training job fails. This is likely due to memory constraints, as the large Llama model cannot fit into a single GPU's memory when processing long sequences from the PubMed dataset.\n",
    "\n",
    "![Without Context Parallelism](./without_cp.png)\n",
    "\n",
    "\n",
    "This experiment highlights the importance of techniques like context parallelism when working with large language models and datasets with long sequences. It allows us to train models that would otherwise be too large to fit in memory, enabling work with more complex models and larger datasets.\n",
    "\n",
    "Context Parallelism with FP8 enabled vs not enabled (On P5)\n",
    "\n",
    "1. **With FP8 Enabled** :\n",
    "\n",
    "![With FP8](./With_FP8.png)\n",
    "\n",
    "2. **Without FP8 Enabled**:\n",
    "\n",
    "![Without FP8](./WO_FP8_P5.png)\n",
    "\n",
    "If you look at the TFLOPS / GPU metric, We can clearly see that with FP8 the throughput is higher\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
