{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune Mixtral 8*7b with PyTorch FSDP and Q-Lora on Amazon SageMaker\n",
    "\n",
    "This notebook explains how you can fine-tune the Mixtral 8*7b model using PyTorch FSDP and Q-Lora with the help of Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index) on Amazon SageMaker. \n",
    "\n",
    "**This notebook is validated and optimized to run on `ml.p4d.2xlarge` instances**\n",
    "\n",
    "**FSDP + Q-Lora Background**\n",
    "\n",
    "Hugging Face shares the support of Q-Lora and PyTorch FSDP (Fully Sharded Data Parallel). FSDP and Q-Lora allow you now to fine-tune Llama, Mistral-like architectures. Hugging Face PEFT is where the core logic resides, read more about it in the [PEFT documentation](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp).\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n",
    "\n",
    "This notebook walks you thorugh how to fine-tune open LLMs from Hugging Face using Amazon SageMaker.\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"py7zr\" \"peft==0.12.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from datasets import load_dataset\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import matplotlib.pyplot as plt\n",
    "from sagemaker.s3 import S3Downloader\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket=None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HF dataset that we will be working with \n",
    "dataset_name=\"gem/viggo\"\n",
    "\n",
    "# Provide hf_token value to acccess mixtral model\n",
    "os.environ['hf_token']=\"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\"\n",
    "os.environ['wandb_token']=\"8cb290b3427fb43b0dcb7e27bd2397ea2fbebede`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "In this example, we use the GEM/viggo dataset from Hugging Face. This is a data-to-text generation dataset in the video game domain. The dataset is clean and organized with about 5,000 data points and the responses are more conversational than information seeking. This type of dataset is ideal for extracting meaningful information from customer reviews. For example, an E-Commerce site like Amazon could use a similarly formatted dataset for fine-tuning a model for NLP analysis to gauge interest in products. Thus, this dataset is a very good candidate for fine-tuning LLMs. To learn more about the viggo dataset, check out this research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = f\"\"\"\n",
    "    Given a target sentence, construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "    This function should describe the target string accurately and the function must be one of the following:\n",
    "    ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute']\n",
    "\n",
    "    The attributes must be one of the following:\n",
    "    ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "    ### Target sentence:\n",
    "    {data_point[\"target\"]}\n",
    "\n",
    "    ### Meaning representation:\n",
    "    {data_point[\"meaning_representation\"]}\n",
    "    \"\"\"\n",
    "    return {\"prompt\": full_prompt.strip()}\n",
    "\n",
    "# Load dataset from the hub\n",
    "train_set = load_dataset(dataset_name, split=\"train\")\n",
    "test_set = load_dataset(dataset_name, split=\"test\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(train_set.features)\n",
    "\n",
    "train_dataset = train_set.map(\n",
    "    generate_and_tokenize_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")\n",
    "\n",
    "test_dataset = test_set.map(\n",
    "    generate_and_tokenize_prompt,\n",
    "    remove_columns=columns_to_remove,\n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Review dataset\n",
    "train_dataset, train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "input_path = f's3://{sess.default_bucket()}/datasets/mixtral'\n",
    "\n",
    "# Save datasets to s3\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "train_dataset.to_json(f\"{input_path}/train/dataset.json\", orient=\"records\")\n",
    "train_dataset_s3_path = f\"{input_path}/train/dataset.json\"\n",
    "test_dataset.to_json(f\"{input_path}/test/dataset.json\", orient=\"records\")\n",
    "test_dataset_s3_path = f\"{input_path}/test/dataset.json\"\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure input length\n",
    "\n",
    "While passing in a dataset to the LLM for fine-tuning, it's important to ensure that the inputs are all of a uniform length. To achieve this, we first visualize the distribution of the input token lengths (or alternatively, firectly find the max length). Based on these results, we identify the maximum input token length, and utilize \"padding\" to ensure all the inputs are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_lengths(train_dataset, test_dataset):\n",
    "    lengths1 = [count_words(x[\"prompt\"]) for x in train_dataset]\n",
    "    lengths2 = [count_words(x[\"prompt\"]) for x in test_dataset]\n",
    "    lengths = lengths1 + lengths2\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(lengths, bins=20, alpha=0.7, color=\"blue\")\n",
    "    plt.xlabel(\"prompt lengths\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Distribution of lengths of input_ids\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_lengths(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print out the max tokens\n",
    "lengths1 = [count_words(x[\"prompt\"]) for x in train_dataset]\n",
    "lengths2 = [count_words(x[\"prompt\"]) for x in test_dataset]\n",
    "lengths = lengths1 + lengths2\n",
    "\n",
    "max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Mixtral 8*7b on Amazon SageMaker\n",
    "\n",
    "We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers`. We prepared a script [launch_fsdp_qlora.py](../scripts/launch_fsdp_qlora.py) which will loads the dataset from disk, prepare the model, tokenizer and start the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. \n",
    "\n",
    "For configuration we use `TrlParser`, that allows us to provide hyperparameters in a yaml file. This `yaml` will be uploaded and provided to Amazon SageMaker similar to our datasets. Below is the config file for fine-tuning Mixtral 8*7b 8B on ml.p4d.24xlarge 40GB GPUs. We are saving the config file as `args.yaml` and upload it to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "hf_token: \"${hf_token}\" # Use HF token to login into Hugging Face to access the Mixtral 8*7b 8b model\n",
    "wandb_token: \"${wandb_token}\"\n",
    "model_id: \"mistralai/Mixtral-8x7B-v0.1\"       # Hugging Face model id\n",
    "max_seq_length: 256  #512 # 2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "\n",
    "output_dir: \"/opt/ml/model/mixtral/adapter\"              # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0003                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                  # number of training epochs\n",
    "per_device_train_batch_size: 100       # batch size per device during training\n",
    "per_device_eval_batch_size: 8         # batch size for evaluation\n",
    "gradient_accumulation_steps: 1        # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "\n",
    "weight_decay: 0.01\n",
    "warmup_steps: 100\n",
    "# offload FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\"\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune LoRA adapter\n",
    "\n",
    "Below estimtor will train the model with QLoRA and will save the LoRA adapter in S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'mixtral-8-7b-finetune'\n",
    "\n",
    "pytorch_estimator = PyTorch(\n",
    "    entry_point= 'launch_fsdp_qlora.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=5800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/args.yaml\" # path to TRL config which was uploaded to s3\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: When using QLoRA, we only train adapters and not the full model. The [launch_fsdp_qlora.py](../scripts/fsdp/run_fsdp_qlora.py) saves the `adapter` at the end of the training to Amazon SageMaker S3 bucket (sagemaker-<region name>-<account_id>)._\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'test': test_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    "\n",
    "# Check input channels configured \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fine the job name of the last run or you can browse the console\n",
    "latest_run_job_name= pytorch_estimator.latest_training_job.job_name\n",
    "latest_run_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge base model with fine-tuned adapter in fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Steps are taken by the next estimator:\n",
    "1. Load base model in fp16 precision\n",
    "2. Convert adapter saved in previous step from fp32 to fp16\n",
    "3. Merge the model\n",
    "4. Run inference both on base model and merged model for comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find S3 path for the last job that ran successfully. You can find this from the SageMaker console \n",
    "\n",
    "# *** Get a job name from the AWS console for the last training run or from the above cell\n",
    "job_name = latest_run_job_name\n",
    "\n",
    "def get_s3_path_from_job_name(job_name):\n",
    "    # Create a Boto3 SageMaker client\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    \n",
    "    # Describe the training job\n",
    "    response = sagemaker_client.describe_training_job(TrainingJobName=job_name)\n",
    "    \n",
    "    # Extract the model artifacts S3 path\n",
    "    model_artifacts_s3_path = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "    \n",
    "    # Extract the output path (this is the general output location)\n",
    "    output_path = response['OutputDataConfig']['S3OutputPath']\n",
    "    \n",
    "    return model_artifacts_s3_path, output_path\n",
    "\n",
    "\n",
    "model_artifacts, output_path = get_s3_path_from_job_name(job_name)\n",
    "\n",
    "\n",
    "print(f\"Model artifacts S3 path: {model_artifacts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adapter_dir_path=f\"{model_artifacts}/mixtral/adapter/\"\n",
    "\n",
    "print(f'\\nAdapter S3 Dir path:{adapter_dir_path} \\n')\n",
    "\n",
    "!aws s3 ls {adapter_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create SageMaker PyTorch Estimator\n",
    "\n",
    "# Define Training Job Name \n",
    "job_name = f'llama3-1-8b-merge-adapter'\n",
    "\n",
    "pytorch_estimator_adapter = PyTorch(\n",
    "    entry_point= 'merge_model_adapter.py',\n",
    "    source_dir=\"./scripts\",\n",
    "    job_name=job_name,\n",
    "    base_job_name=job_name,\n",
    "    max_run=5800,\n",
    "    role=role,\n",
    "    framework_version=\"2.2.0\",\n",
    "    py_version=\"py310\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4d.24xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    hyperparameters={\n",
    "        \"model_id\": \"mistralai/Mixtral-8x7B-v0.1\",  # Hugging Face model id\n",
    "        \"hf_token\": \"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\",\n",
    "        \"dataset_name\":dataset_name\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'adapter': adapter_dir_path,\n",
    "  'testdata': train_dataset_s3_path\n",
    "  }\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "pytorch_estimator_adapter.fit(data, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
